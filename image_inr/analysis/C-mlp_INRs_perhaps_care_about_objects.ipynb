{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e178c98-4b46-4238-800f-a0e3538f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import data_process, helper\n",
    "\n",
    "sys.append('../')\n",
    "\n",
    "from get_mlp_mappings import ComputeMLPContributions\n",
    "from image_vps_datasets import (single_image_cityscape_vps_dataset,\n",
    "                                single_image_vipseg_dataset)\n",
    "from model_all_analysis import ffn, lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33e00bff-f0ba-4d87-93b9-4fda3ffa418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_annotations_to_numpy(tensor_annotations):\n",
    "    annotations = []\n",
    "    \n",
    "    for tensor_anno in tensor_annotations:\n",
    "        annotation = {}\n",
    "\n",
    "        annotation['id'] = tensor_anno['id'].item()\n",
    "        annotation['inst_id'] = tensor_anno['inst_id'].item()\n",
    "        annotation['image_id'] = tensor_anno['image_id'][0] #.item()\n",
    "        annotation['category_id'] = tensor_anno['category_id'].item()\n",
    "        annotation['area'] = tensor_anno['area'].item()\n",
    "        annotation['iscrowd'] = tensor_anno['iscrowd'].item()\n",
    "        annotation['isthing'] = tensor_anno['isthing'].item()\n",
    "\n",
    "        # Convert 'bbox' back to regular format\n",
    "        bbox = [bbox_tensor.item() for bbox_tensor in tensor_anno['bbox']]\n",
    "        annotation['bbox'] = bbox\n",
    "\n",
    "        annotation['binary_mask'] = tensor_anno['binary_mask'].numpy()\n",
    "        \n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def add_other_annotation(annotations):\n",
    "\n",
    "    # Create a mask that will indicate whether a location contains at least one instance\n",
    "    object_region_mask = None\n",
    "    for ann in annotations:\n",
    "        binary_mask = ann['binary_mask'].squeeze()\n",
    "        if object_region_mask is None:\n",
    "            # If the object_region_mask is None, initialize it to current binary_mask otherwise aggregate it\n",
    "            object_region_mask = binary_mask.copy()\n",
    "        else:\n",
    "            object_region_mask += binary_mask\n",
    "\n",
    "    # Binarize\n",
    "    object_region_mask = object_region_mask != 0\n",
    "    \n",
    "    # Create an annotation denoting \"other\" for regions that have no objects\n",
    "    annotations.append({\n",
    "        \"id\": -1,\n",
    "        \"inst_id\": -1,\n",
    "        \"bbox\": compute_bbox(object_region_mask),\n",
    "        \"area\": object_region_mask.sum(),\n",
    "        \"binary_mask\": object_region_mask,\n",
    "        'iscrowd': 0,\n",
    "        'isthing': 0,\n",
    "        'category_id': -1,\n",
    "        'image_id': annotations[0]['image_id']\n",
    "    })\n",
    "    return annotations\n",
    "\n",
    "def plot_image_with_instances(image, annotations, categories_dict, title=None):\n",
    "    plt.rcParams[\"figure.figsize\"] = 15, 10\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for anno in annotations:\n",
    "        # Skip plotting \"other\" regions (regions without objects)\n",
    "        if anno[\"category_id\"] == -1:\n",
    "            continue\n",
    "        # Draw bbox\n",
    "        x, y, w, h = anno[\"bbox\"]\n",
    "\n",
    "        cat_color = np.array(categories_dict[int(anno[\"category_id\"])]['color']) / 255\n",
    "        rectangle = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=cat_color, facecolor='none')\n",
    "        ax.add_patch(rectangle)\n",
    "\n",
    "        if 'binary_mask' in anno.keys():\n",
    "            binary_mask = anno[\"binary_mask\"].squeeze(0)\n",
    "        else:\n",
    "            raise ValueError(\"No binary mask found in annotation\")\n",
    "        # Create a mask where the binary mask is not zero\n",
    "        mask = binary_mask != 0\n",
    "\n",
    "        # Create rgba mask\n",
    "        cmap = ListedColormap(cat_color)\n",
    "        colored_mask = cmap(binary_mask.astype(float) / 1.0)\n",
    "\n",
    "        # Create a mask where the binary mask is not zero\n",
    "        mask = binary_mask != 0\n",
    "\n",
    "        # Set the alpha channel to 0 for regions where the binary mask is zero\n",
    "        colored_mask[:, :, 3] = mask.astype(float)\n",
    "        \n",
    "        # Display the colored mask over the image\n",
    "        ax.imshow(colored_mask, alpha=0.5)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_bbox(binary_mask):\n",
    "    (rows, cols) = np.where(binary_mask > 0)\n",
    "    x_min, x_max, y_min, y_max = min(cols), max(cols), min(rows), max(rows)\n",
    "    # Create the bbox in COCO format [x, y, width, height]\n",
    "    width = x_max - x_min + 1\n",
    "    height = y_max - y_min + 1\n",
    "    bbox = [x_min, y_min, width, height]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83ec5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfg(model_ckpt_dir, dataset_name, vidname):\n",
    "    \n",
    "    if dataset_name == \"cityscapes\":\n",
    "        # Add cityscapes VPS paths\n",
    "        # vidname = \"0005\"\n",
    "        exp_config_path = os.path.join(model_ckpt_dir, 'exp_config.yaml')\n",
    "        \n",
    "        cfg = OmegaConf.load(exp_config_path)\n",
    "        \n",
    "        cfg.data.cityscapes_vps_root = \"../data/cityscapes_vps\"\n",
    "        cfg.data.split = \"val\"\n",
    "        cfg.data.panoptic_video_mask_dir = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"panoptic_video\")\n",
    "        cfg.data.panoptic_inst_mask_dir = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"panoptic_inst\")\n",
    "        \n",
    "        cfg.data.vidname = vidname\n",
    "        # We will work with the first annotated frame in the given video\n",
    "        cfg.data.frame_num_in_video = 0\n",
    "        \n",
    "        cfg.data.data_path = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"img_all\")\n",
    "        cfg.data.anno_path = '../data/cityscapes_vps/panoptic_gt_val_city_vps.json'\n",
    "        \n",
    "        with open(cfg.data.anno_path, 'r') as f:\n",
    "            panoptic_gt_val_city_vps = json.load(f)\n",
    "                    \n",
    "        panoptic_categories = panoptic_gt_val_city_vps['categories']\n",
    "        # panoptic_images = panoptic_gt_val_city_vps['images']\n",
    "        # panoptic_annotations = panoptic_gt_val_city_vps['annotations']    \n",
    "        \n",
    "        categories = panoptic_categories\n",
    "        categories.append(\n",
    "            {'id': -1, 'name': 'other', 'supercategory': '', 'color':None}\n",
    "        )\n",
    "        categories_dict = {el['id']: el for el in categories}\n",
    "\n",
    "    elif dataset_name == \"vipseg\":\n",
    "        exp_config_path = os.path.join(model_ckpt_dir, 'exp_config.yaml')\n",
    "        \n",
    "        \n",
    "        cfg = OmegaConf.load(exp_config_path)\n",
    "        \n",
    "        cfg.data.VIPSeg_720P_root = '../data/VIPSeg-Dataset/VIPSeg/VIPSeg_720P'\n",
    "        cfg.data.panomasks_dir = os.path.join(cfg.data.VIPSeg_720P_root, \"panomasks\")\n",
    "        cfg.data.panomasksRGB_dir = os.path.join(cfg.data.VIPSeg_720P_root, \"panomasksRGB\")\n",
    "        \n",
    "        cfg.data.vidname = vidname\n",
    "        # We will work with the first annotated frame in the given video\n",
    "        cfg.data.frame_num_in_video = 0\n",
    "        \n",
    "        cfg.data.data_path = data_path = os.path.join(cfg.data.VIPSeg_720P_root, \"images\")\n",
    "        cfg.data.anno_path = '../data/VIPSeg-Dataset/VIPSeg/VIPSeg_720P/panoptic_gt_VIPSeg.json'\n",
    "        \n",
    "        # Crop for VIPSeg to match NeRV\n",
    "        cfg.data.crop=[640,1280]\n",
    "        \n",
    "        with open(cfg.data.anno_path, 'r') as f:\n",
    "            panoptic_gt_VIPSeg = json.load(f)\n",
    "                    \n",
    "        panoptic_categories = panoptic_gt_VIPSeg['categories']\n",
    "        # panoptic_videos = panoptic_gt_VIPSeg['videos']\n",
    "        # panoptic_annotations = panoptic_gt_VIPSeg['annotations']    \n",
    "        \n",
    "        categories = panoptic_categories\n",
    "        categories.append(\n",
    "            {'id': -1, 'name': 'other', 'supercategory': '', 'color':None}\n",
    "        )\n",
    "        categories_dict = {el['id']: el for el in categories}\n",
    "        \n",
    "    return cfg, categories_dict\n",
    "\n",
    "# object_categories = [v['name'] for k, v in categories_dict.items()]\n",
    "\n",
    "\n",
    "def load_model(cfg):\n",
    "    save_dir = cfg.logging.checkpoint.logdir\n",
    "    ckpt_path = helper.find_ckpt(save_dir)\n",
    "    print(f'Loading checkpoint from {ckpt_path}')\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "    # Load checkpoint into this wrapper model cause that is what is stored in disk :)\n",
    "    model = lightning_model(cfg, ffn(cfg))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    ffn_model = model.model\n",
    "    \n",
    "    return ffn_model.cuda()\n",
    "\n",
    "def get_loader(cfg,dataset_name,val=False):\n",
    "    # use the dataloader which returns image along with annotations\n",
    "    if dataset_name == \"cityscapes\":\n",
    "        img_dataset = single_image_cityscape_vps_dataset(cfg)\n",
    "    else:\n",
    "        img_dataset = single_image_vipseg_dataset(cfg)\n",
    "    #create torch dataset for one image.\n",
    "    loader = DataLoader(img_dataset, batch_size=1, shuffle = False ,num_workers=0)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e61d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple videos\n",
    "dataset_names = ['cityscapes', 'vipseg']\n",
    "vidnames = {\n",
    "    'cityscapes': ['0005', '0175'],\n",
    "    'vipseg': ['12_n-ytHkMceew', '26_cblDl5vCZnw']\n",
    "}\n",
    "\n",
    "vid_data_folder_name = {\n",
    "    \"cityscapes\": \"Cityscapes_VPS_models\",\n",
    "    \"vipseg\": \"VIPSeg_models\"\n",
    "}\n",
    "\n",
    "\n",
    "cfg_dict = {}\n",
    "dataloader_dict = {}\n",
    "weights_dict = {}\n",
    "ffn_models_dict = {}\n",
    "categories_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    weights_dict[dataset_name] = {}\n",
    "    cfg_dict[dataset_name] = {}\n",
    "    ffn_models_dict[dataset_name] = {}\n",
    "    categories_dicts[dataset_name] = {}\n",
    "\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        vid_data_folder = vid_data_folder_name[dataset_name]\n",
    "        weights_dict[dataset_name][vidname] = f'output/{vid_data_folder}/{vidname}/{vidname}_framenum_0_128_256'\n",
    "        \n",
    "        cfg, categories_dict = load_cfg(weights_dict[dataset_name][vidname], dataset_name, vidname)\n",
    "        cfg_dict[dataset_name][vidname] = cfg\n",
    "        categories_dicts[dataset_name][vidname] = categories_dict\n",
    "        \n",
    "        \n",
    "        ffn_models_dict[dataset_name][vidname] = load_model(cfg)\n",
    "        \n",
    "for dataset_name in dataset_names:\n",
    "    dataloader_dict[dataset_name] = {}\n",
    "    \n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = get_loader(cfg_dict[dataset_name][vidname], dataset_name)\n",
    "        \n",
    "        dataloader_dict[dataset_name][vidname] = single_image_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74e6fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_results(single_image_dataloader, ffn_model, cfg, categories_dict):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(single_image_dataloader))\n",
    "\n",
    "    data = batch['data'].cuda()\n",
    "    N,C,H,W = data.shape\n",
    "    annotations = convert_tensor_annotations_to_numpy(batch['annotations'])\n",
    "    annotations = add_other_annotation(annotations)\n",
    "\n",
    "    features = batch['features'].squeeze().cuda()\n",
    "    features_shape = batch['features_shape'].squeeze().tolist()\n",
    "    reshape = True\n",
    "\n",
    "    proc = data_process.DataProcessor(cfg.data, device='cpu')\n",
    "    x = batch['data']\n",
    "    coords = proc.get_coordinates(data_shape=features_shape,patch_shape=cfg.data.patch_shape,\\\n",
    "                                    split=cfg.data.coord_split,normalize_range=cfg.data.coord_normalize_range)\n",
    "    coords = coords.to(x).cuda()\n",
    "\n",
    "    # Create a dictionary to store the intermediate decoder_results from each seeded model, over time.\n",
    "    inference_results = {}\n",
    "    kwargs = {}\n",
    "    with torch.no_grad():\n",
    "        out = ffn_model(coords, img=data)\n",
    "        pred = out['predicted']\n",
    "        intermediate_results = out[\"intermediate_results\"]\n",
    "        \n",
    "        if reshape:\n",
    "            # This reshapes the prediction into an image\n",
    "            pred = proc.process_outputs(\n",
    "                pred,input_img_shape=batch['data_shape'].squeeze().tolist(),\\\n",
    "                features_shape=features_shape,\\\n",
    "                patch_shape=cfg.data.patch_shape)\n",
    "\n",
    "    inference_results = {\n",
    "        \"data\": batch[\"data\"],\n",
    "        \"pred\": pred,\n",
    "        \"annotations\": annotations,\n",
    "        \"img_hw\": (H,W),\n",
    "        \"intermediate_results\": intermediate_results\n",
    "    }\n",
    "    \n",
    "    categories_in_frame = {}\n",
    "    for ann in annotations:\n",
    "        if ann[\"category_id\"] not in categories_in_frame:\n",
    "            categories_in_frame[ann[\"category_id\"]] = categories_dict[ann[\"category_id\"]]\n",
    "\n",
    "    categories_in_frame[-1] = categories_dict[-1]\n",
    "    object_categories = [v['name'] for k, v in categories_in_frame.items()]\n",
    "    categories_in_frame = [v for k, v in categories_in_frame.items()]\n",
    "    \n",
    "    return inference_results, categories_in_frame, object_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_info(inference_results, object_categories, categories):\n",
    "    \n",
    "    # Create a map from unique inst_id to a suffix that denotes an instance number in current video. Also stores object category.\n",
    "    inst_id_to_cat_and_inst_suffix = {}\n",
    "    \n",
    "    object_to_instances_map = {}\n",
    "    obj_to_obj_name_idx = {}\n",
    "    \n",
    "    instance_names = []\n",
    "    object_to_instances_map = defaultdict(list)\n",
    "    \n",
    "    for idx, object_cat in enumerate(object_categories):\n",
    "        obj_to_obj_name_idx[object_cat] = idx\n",
    "    \n",
    "    instance_to_ann_id_map = {}\n",
    "\n",
    "    # Get annos for current frame\n",
    "    frame_annos = inference_results[\"annotations\"]\n",
    "    for ann in frame_annos:\n",
    "        category_name = [cat[\"name\"] for cat in categories if cat[\"id\"] == ann[\"category_id\"]][0]\n",
    "        \n",
    "        # Get the current number of instances of this category            \n",
    "        num_instances_of_obj = len(object_to_instances_map[category_name])\n",
    "        \n",
    "        if ann[\"inst_id\"] not in list(inst_id_to_cat_and_inst_suffix.keys()):\n",
    "            # Create a dictionary for the instance\n",
    "            inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]] = {\n",
    "                \"category\": category_name,\n",
    "                \"inst_suffix\": num_instances_of_obj, #0\n",
    "                \"instance_name\": category_name + '_' + str(num_instances_of_obj)\n",
    "            }\n",
    "\n",
    "        # Retrieve the stored instance name\n",
    "        instance_name = inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]][\"instance_name\"]\n",
    "\n",
    "        instance_to_ann_id_map[instance_name] = ann['id']\n",
    "\n",
    "        if instance_name not in instance_names:\n",
    "            object_to_instances_map[category_name].append(instance_name)\n",
    "            instance_names.append(instance_name)\n",
    "\n",
    "    def custom_sort_key(item):\n",
    "        parts = item.split('_')\n",
    "        return (\"_\".join(parts[:-1]), int(parts[-1]))\n",
    "        \n",
    "    # Sort the instance names\n",
    "    instance_names = [item for item in sorted(instance_names, key=custom_sort_key)]\n",
    "    \n",
    "    # Find \"other_0\" instance in this list and move it to the back\n",
    "    instance_names.append(instance_names.pop(instance_names.index(\"other_0\")))\n",
    "    \n",
    "    return inst_id_to_cat_and_inst_suffix, instance_to_ann_id_map, instance_names, object_to_instances_map, obj_to_obj_name_idx, instance_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffc28a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each instance - get average contrib, total contrib and total area (other useful info too)\n",
    "def get_instance_contribs(\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, annotations, instance_to_ann_id_map, instance_names \n",
    "):\n",
    "    \n",
    "    # Maps for kernel to object contributions\n",
    "    num_layer_1_weights = layer_1_output_contrib.shape[0]\n",
    "    num_layer_2_weights = layer_2_output_contrib.shape[0]\n",
    "    num_layer_3_weights = layer_3_output_contrib.shape[0]\n",
    "\n",
    "    num_instances = len(instance_names)\n",
    "    layer_1_to_instance_contribs = torch.zeros((num_layer_1_weights, num_instances))\n",
    "    layer_2_to_instance_contribs = torch.zeros((num_layer_2_weights, num_instances))\n",
    "    layer_3_to_instance_contribs = torch.zeros((num_layer_3_weights, num_instances))\n",
    "\n",
    "    for instance in instance_to_ann_id_map:\n",
    "        ann_id = instance_to_ann_id_map[instance]\n",
    "        ann = [ann for ann in annotations if ann['id'] == ann_id][0]\n",
    "        \n",
    "        area = ann['area']\n",
    "        binary_mask = ann['binary_mask'].squeeze()\n",
    "        \n",
    "        # Use binary mask of shape hxw to index into the n_featsxhxw contribution tensor\n",
    "        # to get the contribs for the current instance\n",
    "        curr_instance_layer_1_contribs = torch.abs(layer_1_output_contrib[:, binary_mask])\n",
    "        curr_instance_layer_2_contribs = torch.abs(layer_2_output_contrib[:, binary_mask])\n",
    "        curr_instance_layer_3_contribs = torch.abs(layer_3_output_contrib[:, binary_mask])\n",
    "        \n",
    "        # Get aggregated total contribution for each kernel to the instance\n",
    "        total_layer_1_contrib = torch.sum(curr_instance_layer_1_contribs, dim=-1)\n",
    "        total_layer_2_contrib = torch.sum(curr_instance_layer_2_contribs, dim=-1)\n",
    "        total_layer_3_contrib = torch.sum(curr_instance_layer_3_contribs, dim=-1)\n",
    "        \n",
    "        avg_layer_1_contrib = total_layer_1_contrib / area\n",
    "        avg_layer_2_contrib = total_layer_2_contrib / area\n",
    "        avg_layer_3_contrib = total_layer_3_contrib / area\n",
    "            \n",
    "        # Store the average contribution from each head kernel to current instance\n",
    "        inst_idx = instance_names.index(instance)\n",
    "        layer_1_to_instance_contribs[:, inst_idx] = avg_layer_1_contrib.flatten()\n",
    "    \n",
    "        # Store the average contribution from each block 3 kernel to current instance\n",
    "        layer_2_to_instance_contribs[:, inst_idx] = avg_layer_2_contrib.flatten()\n",
    "\n",
    "        layer_3_to_instance_contribs[:, inst_idx] = avg_layer_3_contrib.flatten()\n",
    "        \n",
    "\n",
    "    return layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1c820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_contribs(layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs):\n",
    "    # To deal with dead (all 0 contrib) neurons, we need to be careful about normalization\n",
    "    \n",
    "    layer_1_nonzero_rows = (torch.sum(layer_1_to_instance_contribs, dim=1)) != 0\n",
    "    layer_2_nonzero_rows = (torch.sum(layer_2_to_instance_contribs, dim=1)) != 0\n",
    "    layer_3_nonzero_rows = (torch.sum(layer_3_to_instance_contribs, dim=1)) != 0\n",
    "    \n",
    "    # Remove the rows (kernels) whose contributions sum to zeros\n",
    "    layer_1_to_instance_contribs = layer_1_to_instance_contribs[layer_1_nonzero_rows, :]\n",
    "    layer_2_to_instance_contribs = layer_2_to_instance_contribs[layer_2_nonzero_rows, :]\n",
    "    layer_3_to_instance_contribs = layer_3_to_instance_contribs[layer_3_nonzero_rows, :]\n",
    "    \n",
    "    layer_1_contribs_normalized_by_instance = layer_1_to_instance_contribs / torch.sum(layer_1_to_instance_contribs, dim=1)[:, None]\n",
    "    layer_2_contribs_normalized_by_instance = layer_2_to_instance_contribs / torch.sum(layer_2_to_instance_contribs, dim=1)[:, None]\n",
    "    layer_3_contribs_normalized_by_instance = layer_3_to_instance_contribs / torch.sum(layer_3_to_instance_contribs, dim=1)[:, None]\n",
    "     \n",
    "    return layer_1_contribs_normalized_by_instance, layer_2_contribs_normalized_by_instance, layer_3_contribs_normalized_by_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d4fdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and stuff\n",
    "def compute_all_variables_for_frame(inference_results, ffn_model):\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    (H,W) = inference_results[\"img_hw\"]\n",
    "    annotations = inference_results[\"annotations\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeMLPContributions(\n",
    "        ffn_model, intermediate_results, (H,W)\n",
    "    )\n",
    "\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, _, _, _ = compute_contrib_obj.compute_all_layer_mappings()\n",
    "\n",
    "    layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs \\\n",
    "            = get_instance_contribs(layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, annotations, instance_to_ann_id_map, instance_names)\n",
    "\n",
    "    # Beware, some of the neurons in MLP are dead (all 0 contribs). These are removed in normalization\n",
    "\n",
    "    layer_1_contribs_normalized_by_instance, layer_2_contribs_normalized_by_instance, layer_3_contribs_normalized_by_instance \\\n",
    "            = get_normalized_contribs(layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs)\n",
    "\n",
    "    all_variables_for_frame = {\n",
    "        \"instance_names\": instance_names,\n",
    "        \"layer_1_contribs_normalized_by_instance\": layer_1_contribs_normalized_by_instance,\n",
    "        \"layer_2_contribs_normalized_by_instance\": layer_2_contribs_normalized_by_instance,\n",
    "        \"layer_3_contribs_normalized_by_instance\": layer_3_contribs_normalized_by_instance\n",
    "    }\n",
    "\n",
    "    return all_variables_for_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8ea38",
   "metadata": {},
   "source": [
    "# MLP Contribs to Instances (Scatter Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94f81028",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = [\n",
    "    '#1f77b4', '#2ca02c', '#d62728', '#e377c2', '#7f7f7f',\n",
    "    '#843c39', '#7b4173', '#5254a3', '#fdc7c7', '#637939',\n",
    "    '#bcbd22', '#17becf', '#b1c2b3', '#ff9896', '#ff7f0e',\n",
    "    '#f5c651', '#02a3a3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9faaf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel_instance_contribs(all_variables_for_frame, obj_to_obj_name_idx, custom_colors):\n",
    "    num_instances_per_obj = 3\n",
    "    num_neuron_samples_per_layer = 3\n",
    "\n",
    "    layer_1_contribs_normalized_by_instance = all_variables_for_frame[\"layer_1_contribs_normalized_by_instance\"]\n",
    "    layer_2_contribs_normalized_by_instance = all_variables_for_frame[\"layer_2_contribs_normalized_by_instance\"]\n",
    "    layer_3_contribs_normalized_by_instance = all_variables_for_frame[\"layer_3_contribs_normalized_by_instance\"]\n",
    "    \n",
    "    sampled_layer_1_neurons = torch.randperm(layer_1_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "    sampled_layer_2_neurons = torch.randperm(layer_2_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "    sampled_layer_3_neurons = torch.randperm(layer_3_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "\n",
    "    layer_to_sampled_neurons = {\n",
    "        1: sampled_layer_1_neurons,\n",
    "        2: sampled_layer_2_neurons,\n",
    "        3: sampled_layer_3_neurons\n",
    "    }\n",
    "\n",
    "    object_to_color_map = dict(zip(object_categories, custom_colors[:len(object_categories)]))\n",
    "        \n",
    "    markers = [\".\", \"*\", \"p\", \"X\", \"^\", \"s\", \"2\", \"d\"]\n",
    "\n",
    "    instance_names = all_variables_for_frame[\"instance_names\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,4), tight_layout=True)\n",
    "\n",
    "    x = np.arange(num_neuron_samples_per_layer * 3)\n",
    "    # or define offsets in terms of objects\n",
    "    x_offsets = np.linspace(-0.25, 0.25, len(obj_to_obj_name_idx.keys()))\n",
    "\n",
    "    # Get contributions of each instance from each sampled kernel\n",
    "    inst_kernel_contribs = torch.zeros((len(instance_names), num_neuron_samples_per_layer*3))\n",
    "\n",
    "    # Plot neurons for each layer\n",
    "    for layer_idx, layer in enumerate([1, 2, 3]):\n",
    "        \n",
    "        for neuron_idx, sampled_neuron_idx in enumerate(layer_to_sampled_neurons[layer]):\n",
    "            \n",
    "            if layer == 1:\n",
    "                sampled_layer_contribs = layer_1_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "            elif layer == 2:\n",
    "                sampled_layer_contribs = layer_2_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "            elif layer == 3:\n",
    "                sampled_layer_contribs = layer_3_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "                \n",
    "            inst_kernel_contribs[:, (layer_idx-1)*3 + neuron_idx] = sampled_layer_contribs.squeeze()\n",
    " \n",
    "    # Plot each instances line, using a different marker for each instance in a category\n",
    "    leading_obj = '_'.join(instance_names[0].split('_')[0:-1])\n",
    "    marker_idx = -1\n",
    "    for inst_num, inst_name in enumerate(instance_names):\n",
    "        # reset marker index for each new object\n",
    "        if('_'.join(inst_name.split('_')[0:-1]) == leading_obj):\n",
    "            marker_idx += 1\n",
    "        else:\n",
    "            marker_idx = 0\n",
    "            leading_obj = '_'.join(inst_name.split('_')[0:-1])\n",
    "        \n",
    "        if marker_idx < num_instances_per_obj:\n",
    "            data_with_nones = np.where(inst_kernel_contribs[inst_num, :] == 0.0, None, inst_kernel_contribs[inst_num, :]\n",
    "            # Offset instances of each object by a certain amount\n",
    "            offset_for_obj = x_offsets[list(obj_to_obj_name_idx.keys()).index(leading_obj)]\n",
    "            label = inst_name.replace(' ', '_')\n",
    "            label = label.replace('vegitation', 'vegetation')\n",
    "            ax.plot(x + offset_for_obj, data_with_nones, marker=markers[marker_idx], linestyle='None', label=label, color=object_to_color_map[leading_obj])\n",
    "            \n",
    "            \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['a', 'b', 'c', 'p', 'q', 'r', 'x', 'y', 'z', ])\n",
    "    \n",
    "        \n",
    "    fig.suptitle(f\"MLP Layers - Instance Contributions for Sampled Neurons\", y=1.15)\n",
    "    ax.set_ylabel(f\"Contributions\")\n",
    "    ax.set_xlabel(f\"Sampled Neurons from Layers 1, 2, 3\")\n",
    "\n",
    "    # Add vertical lines between the 3rd and 4th x-ticks and between the 6th and 7th x-ticks\n",
    "    ax.axvline(x=2.5, linestyle='--', color='gray', alpha=0.4, linewidth=1)\n",
    "    ax.axvline(x=5.5, linestyle='--', color='gray', alpha=0.4, linewidth=1)\n",
    "\n",
    "    # Every subplot has the same legend, I want to pick one and then plot that for the entire figure\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=6, bbox_to_anchor=(0.5, 1.10), fontsize='small')\n",
    "\n",
    "\n",
    "    neuron_to_inst_contrib_dict = {\n",
    "        \"instance_names\": instance_names,\n",
    "        \"object_categories\": object_categories,\n",
    "        \"obj_to_obj_name_idx\": obj_to_obj_name_idx,\n",
    "        \n",
    "        \"num_instances_per_obj\": num_instances_per_obj,\n",
    "        \"num_neuron_samples_per_layer\": num_neuron_samples_per_layer,\n",
    "        \"inst_kernel_contribs\": inst_kernel_contribs,\n",
    "    }\n",
    "    return neuron_to_inst_contrib_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282e7f2",
   "metadata": {},
   "source": [
    "## Trying different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_neuron_to_inst_contrib_dict = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = dataloader_dict[dataset_name][vidname]\n",
    "        ffn_model = ffn_models_dict[dataset_name][vidname]\n",
    "        categories_dict = categories_dicts[dataset_name][vidname]\n",
    "        cfg = cfg_dict[dataset_name][vidname]\n",
    "\n",
    "        categories = list(categories_dict.values())\n",
    "        \n",
    "\n",
    "        # inference_results = compute_inference_results(single_image_dataloader, ffn_model, cfg, categories_dict)\n",
    "\n",
    "        inference_results, categories_in_frame, object_categories = compute_inference_results(\n",
    "            single_image_dataloader, ffn_model, cfg, categories_dict\n",
    "        )\n",
    "        \n",
    "        inst_id_to_cat_and_inst_suffix, instance_to_ann_id_map, instance_names, object_to_instances_map, \\\n",
    "            obj_to_obj_name_idx, instance_names = get_instance_info(inference_results, object_categories, categories)\n",
    "        \n",
    "        all_variables_for_frame = compute_all_variables_for_frame(inference_results, ffn_model)\n",
    "        per_vid_neuron_to_inst_contrib_dict[vidname] = plot_kernel_instance_contribs(all_variables_for_frame, obj_to_obj_name_idx, custom_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9efda",
   "metadata": {},
   "source": [
    "### Save dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311b06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = '../plotting_source_data/MLP/C-INRs_perhaps_care_about_objects'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, f\"per_vid_neuron_to_inst_contrib_dict.pkl\"), 'wb') as f:\n",
    "    pickle.dump(per_vid_neuron_to_inst_contrib_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669452a-925d-4a1c-8b38-37d124ad1525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
