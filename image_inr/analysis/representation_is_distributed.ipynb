{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80bb3eff-e5fe-4b55-91e8-40500cda3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils import data_process\n",
    "\n",
    "from analysis_utils import *\n",
    "\n",
    "sys.append('../')\n",
    "\n",
    "from get_mlp_mappings import ComputeMLPContributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple videos\n",
    "dataset_names = ['cityscapes', 'vipseg']\n",
    "vidnames = {\n",
    "    'cityscapes': ['0005', '0175'],\n",
    "    'vipseg': ['12_n-ytHkMceew', '26_cblDl5vCZnw']\n",
    "}\n",
    "\n",
    "vid_data_folder_name = {\n",
    "    \"cityscapes\": \"Cityscapes_VPS_models\",\n",
    "    \"vipseg\": \"VIPSeg_models\"\n",
    "}\n",
    "\n",
    "\n",
    "cfg_dict = {}\n",
    "dataloader_dict = {}\n",
    "weights_dict = {}\n",
    "ffn_models_dict = {}\n",
    "categories_dicts = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    weights_dict[dataset_name] = {}\n",
    "    cfg_dict[dataset_name] = {}\n",
    "    ffn_models_dict[dataset_name] = {}\n",
    "    categories_dicts[dataset_name] = {}\n",
    "\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        vid_data_folder = vid_data_folder_name[dataset_name]\n",
    "        weights_dict[dataset_name][vidname] = f'output/{vid_data_folder}/{vidname}/{vidname}_framenum_0_128_256'\n",
    "        \n",
    "        cfg, categories_dict = load_cfg(weights_dict[dataset_name][vidname], dataset_name, vidname)\n",
    "        cfg_dict[dataset_name][vidname] = cfg\n",
    "        categories_dicts[dataset_name][vidname] = categories_dict\n",
    "        ffn_models_dict[dataset_name][vidname] = load_model(cfg)\n",
    "        \n",
    "for dataset_name in dataset_names:\n",
    "    dataloader_dict[dataset_name] = {}\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = get_loader(cfg_dict[dataset_name][vidname], dataset_name)\n",
    "        dataloader_dict[dataset_name][vidname] = single_image_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b6eea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_results(single_image_dataloader, ffn_model, cfg):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(single_image_dataloader))\n",
    "\n",
    "    data = batch['data'].cuda()\n",
    "    N,C,H,W = data.shape\n",
    "    annotations = convert_annotations_to_numpy(batch['annotations'])\n",
    "    annotations = add_other_annotation(annotations)\n",
    "\n",
    "    features = batch['features'].squeeze().cuda()\n",
    "    features_shape = batch['features_shape'].squeeze().tolist()\n",
    "    reshape = True\n",
    "    proc = data_process.DataProcessor(cfg.data, device='cpu')\n",
    "    x = batch['data']\n",
    "    coords = proc.get_coordinates(data_shape=features_shape,patch_shape=cfg.data.patch_shape,\\\n",
    "                                    split=cfg.data.coord_split,normalize_range=cfg.data.coord_normalize_range)\n",
    "    coords = coords.to(x).cuda()\n",
    "\n",
    "    inference_results = {}\n",
    "    with torch.no_grad():\n",
    "        out = ffn_model(coords, img=data)\n",
    "        pred = out['predicted']\n",
    "        intermediate_results = out[\"intermediate_results\"]\n",
    "        \n",
    "        if reshape:\n",
    "            # This reshapes the prediction into an image\n",
    "            pred = proc.process_outputs(\n",
    "                pred,input_img_shape=batch['data_shape'].squeeze().tolist(),\\\n",
    "                features_shape=features_shape,\\\n",
    "                patch_shape=cfg.data.patch_shape)\n",
    "\n",
    "    inference_results = {\n",
    "        \"data\": batch[\"data\"],\n",
    "        \"pred\": pred,\n",
    "        \"annotations\": annotations,\n",
    "        \"img_hw\": (H,W),\n",
    "        \"intermediate_results\": intermediate_results\n",
    "    }\n",
    "    \n",
    "    return inference_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8236f0b",
   "metadata": {},
   "source": [
    "# Pixels Per Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9481b80e-c2e2-40e8-a4ff-2f2ad32ec05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixels_per_neuron(inference_results, ffn_model):\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    (H,W) = inference_results[\"img_hw\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeMLPContributions(\n",
    "        ffn_model, intermediate_results, (H,W)\n",
    "    )\n",
    "\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, _, _, _ = compute_contrib_obj.compute_all_layer_mappings()\n",
    "\n",
    "    layer_1_contrib_abs = torch.abs(layer_1_output_contrib).flatten(1,2)\n",
    "    layer_2_contrib_abs = torch.abs(layer_2_output_contrib).flatten(1,2)\n",
    "    layer_3_contrib_abs = torch.abs(layer_3_output_contrib).flatten(1,2)\n",
    "    # Sum contribs across neurons for each pixel\n",
    "    layer_1_contribs_fraction = layer_1_contrib_abs / layer_1_contrib_abs.sum(dim=0)\n",
    "    layer_2_contribs_fraction = layer_2_contrib_abs / layer_2_contrib_abs.sum(dim=0)\n",
    "    layer_3_contribs_fraction = layer_3_contrib_abs / layer_3_contrib_abs.sum(dim=0)\n",
    "    \n",
    "    num_pixels_with_meaningful_contrib_dict = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "        \n",
    "    layer_1_num_pixels_with_meaningful_contrib = (layer_1_contribs_fraction > (1 / layer_1_output_contrib.size(0))).sum(dim=1)\n",
    "    layer_2_num_pixels_with_meaningful_contrib = (layer_2_contribs_fraction > (1 / layer_2_output_contrib.size(0))).sum(dim=1)\n",
    "    layer_3_num_pixels_with_meaningful_contrib = (layer_3_contribs_fraction > (1 / layer_3_output_contrib.size(0))).sum(dim=1)\n",
    "    layer_1_num_pixels_with_meaningful_contrib, _ = torch.sort(layer_1_num_pixels_with_meaningful_contrib)\n",
    "    layer_2_num_pixels_with_meaningful_contrib, _ = torch.sort(layer_2_num_pixels_with_meaningful_contrib)\n",
    "    layer_3_num_pixels_with_meaningful_contrib, _ = torch.sort(layer_3_num_pixels_with_meaningful_contrib)\n",
    "    \n",
    "    axs.plot(layer_3_num_pixels_with_meaningful_contrib, label=\"Layer 3\")\n",
    "    axs.plot(layer_2_num_pixels_with_meaningful_contrib, label=\"Layer 2\")\n",
    "    axs.plot(layer_1_num_pixels_with_meaningful_contrib, label=\"Layer 1\")\n",
    "    \n",
    "    num_pixels_with_meaningful_contrib_dict = {\n",
    "        \"layer_1\": layer_1_num_pixels_with_meaningful_contrib,\n",
    "        \"layer_2\": layer_2_num_pixels_with_meaningful_contrib,\n",
    "        \"layer_3\": layer_3_num_pixels_with_meaningful_contrib\n",
    "    }\n",
    "    \n",
    "    plt.legend()\n",
    "    axs.set_title(f\"Threshold 1/num_neurons\")\n",
    "    plt.show()\n",
    "    \n",
    "    return num_pixels_with_meaningful_contrib_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c27eb-b15f-406c-9d2a-857c281b34b9",
   "metadata": {},
   "source": [
    "# Neurons Per Pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrib_thresh_using_auc(abs_contrib_map , target_area=0.05):\n",
    "    total_sum = abs_contrib_map.sum()\n",
    "    cutoff_contrib_sum = total_sum * (1 - target_area)\n",
    "\n",
    "    sorted_contributions = abs_contrib_map.flatten()\n",
    "    sorted_indices = torch.argsort(sorted_contributions, descending=True)\n",
    "    cum_sum = torch.cumsum(sorted_contributions[sorted_indices], dim=0)\n",
    "\n",
    "    idx = torch.nonzero(cum_sum >= cutoff_contrib_sum, as_tuple=False)[0, 0].item()\n",
    "    chosen_thresh = sorted_contributions[sorted_indices][idx]\n",
    "    \n",
    "    return chosen_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa4ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neurons_per_pixel_heatmap(inference_results, ffn_model):\n",
    "    gt_img = inference_results[\"data\"][0]\n",
    "    pred = inference_results[\"pred\"]\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    (H,W) = inference_results[\"img_hw\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeMLPContributions(\n",
    "        ffn_model, intermediate_results, (H,W)\n",
    "    )\n",
    "\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, _, _, _ = compute_contrib_obj.compute_all_layer_mappings()\n",
    "    layer_1_contrib_abs = torch.abs(layer_1_output_contrib)\n",
    "    layer_2_contrib_abs = torch.abs(layer_2_output_contrib)\n",
    "    layer_3_contrib_abs = torch.abs(layer_3_output_contrib)\n",
    "        \n",
    "    target_areas = [0.1, 0.5]\n",
    "    num_kernels_with_meaningful_contrib = {}\n",
    "        \n",
    "    for target_area in target_areas:\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        \n",
    "        # Compute layerwise threshold contributions\n",
    "        layer_1_thresh = compute_contrib_thresh_using_auc(abs_contrib_map = layer_1_contrib_abs, target_area = target_area)\n",
    "        layer_2_thresh = compute_contrib_thresh_using_auc(abs_contrib_map = layer_2_contrib_abs, target_area = target_area) \n",
    "        layer_3_thresh = compute_contrib_thresh_using_auc(abs_contrib_map = layer_3_contrib_abs, target_area = target_area) \n",
    "        \n",
    "        # Obtain as percentage of total kernels\n",
    "        layer_1_num_kernels_with_meaningful_contrib = (layer_1_contrib_abs > layer_1_thresh).sum(dim=0) / layer_1_contrib_abs.size(0)\n",
    "        layer_2_num_kernels_with_meaningful_contrib = (layer_2_contrib_abs > layer_2_thresh).sum(dim=0) / layer_2_contrib_abs.size(0)\n",
    "        layer_3_num_kernels_with_meaningful_contrib = (layer_2_contrib_abs > layer_3_thresh).sum(dim=0) / layer_3_contrib_abs.size(0)\n",
    "        \n",
    "        axs[0].imshow(gt_img.permute(1,2,0).cpu().numpy())\n",
    "        axs[0].set_title(\"Ground Truth Image\")\n",
    "        axs[1].set_title(f\"Layer 3 Heatmap - Threshold={layer_1_thresh:.4f}\")\n",
    "        axs[2].set_title(f\"Layer 2 Heatmap - Threshold={layer_2_thresh:.4f}\")\n",
    "        axs[3].set_title(f\"Layer 1 Heatmap - Threshold={layer_3_thresh:.4f}\")\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "            \n",
    "        fig.suptitle(f\"Neurons per Pixel Heatmap (Contributions above Target Area Under Sorted Curve {target_area*100}%)\", y=0.7)\n",
    "        plt.show()\n",
    "        \n",
    "        num_kernels_with_meaningful_contrib[target_area] = {\n",
    "            \"layer_1\": layer_1_num_kernels_with_meaningful_contrib,\n",
    "            \"layer_2\": layer_2_num_kernels_with_meaningful_contrib,\n",
    "            \"layer_3\": layer_3_num_kernels_with_meaningful_contrib\n",
    "        }\n",
    "        \n",
    "    return num_kernels_with_meaningful_contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f5105",
   "metadata": {},
   "source": [
    "Analyze each image and save the raw values for downstream visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_num_pixels_with_meaningful_contrib = {}\n",
    "per_vid_num_kernels_with_meaningful_contrib = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = dataloader_dict[dataset_name][vidname]\n",
    "        ffn_model = ffn_models_dict[dataset_name][vidname]\n",
    "        cfg = cfg_dict[dataset_name][vidname]\n",
    "\n",
    "        inference_results = compute_inference_results(single_image_dataloader, ffn_model, cfg)\n",
    "        per_vid_num_pixels_with_meaningful_contrib[vidname] = plot_pixels_per_neuron(inference_results, ffn_model)\n",
    "        per_vid_num_kernels_with_meaningful_contrib[vidname] = plot_neurons_per_pixel_heatmap(inference_results, ffn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db59e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = '../analysis_data/MLP/representation_is_distributed'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, f\"per_vid_num_pixels_with_meaningful_contrib.pkl\"), 'wb') as f:\n",
    "    pickle.dump(per_vid_num_pixels_with_meaningful_contrib, f)\n",
    "with open(os.path.join(save_dir, f\"per_vid_num_kernels_with_meaningful_contrib.pkl\"), 'wb') as f:\n",
    "    pickle.dump(per_vid_num_kernels_with_meaningful_contrib, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
