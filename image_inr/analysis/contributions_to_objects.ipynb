{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e178c98-4b46-4238-800f-a0e3538f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import data_process\n",
    "\n",
    "from analysis_utils import *\n",
    "\n",
    "sys.append(\"../\")\n",
    "\n",
    "from get_mlp_mappings import ComputeMLPContributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple videos\n",
    "dataset_names = [\"cityscapes\", \"vipseg\"]\n",
    "vidnames = {\n",
    "    \"cityscapes\": [\"0005\", \"0175\"],\n",
    "    \"vipseg\": [\"12_n-ytHkMceew\", \"26_cblDl5vCZnw\"],\n",
    "}\n",
    "\n",
    "\n",
    "cfg_dict = {}\n",
    "dataloader_dict = {}\n",
    "weights_dict = {}\n",
    "ffn_models_dict = {}\n",
    "categories_dicts = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    weights_dict[dataset_name] = {}\n",
    "    cfg_dict[dataset_name] = {}\n",
    "    ffn_models_dict[dataset_name] = {}\n",
    "    categories_dicts[dataset_name] = {}\n",
    "\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        weights_dict[dataset_name][vidname] = \"path/to/checkpoint/\"\n",
    "\n",
    "        cfg, categories_dict = load_cfg(\n",
    "            weights_dict[dataset_name][vidname], dataset_name, vidname\n",
    "        )\n",
    "        cfg_dict[dataset_name][vidname] = cfg\n",
    "        categories_dicts[dataset_name][vidname] = categories_dict\n",
    "        ffn_models_dict[dataset_name][vidname] = load_model(cfg)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataloader_dict[dataset_name] = {}\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = get_loader(\n",
    "            cfg_dict[dataset_name][vidname], dataset_name\n",
    "        )\n",
    "        dataloader_dict[dataset_name][vidname] = single_image_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74e6fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_results(single_image_dataloader, ffn_model, cfg, categories_dict):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(single_image_dataloader))\n",
    "\n",
    "    data = batch[\"data\"].cuda()\n",
    "    N, C, H, W = data.shape\n",
    "    annotations = convert_annotations_to_numpy(batch[\"annotations\"])\n",
    "    annotations = add_other_annotation(annotations)\n",
    "\n",
    "    features = batch[\"features\"].squeeze().cuda()\n",
    "    features_shape = batch[\"features_shape\"].squeeze().tolist()\n",
    "    reshape = True\n",
    "    proc = data_process.DataProcessor(cfg.data, device=\"cpu\")\n",
    "    x = batch[\"data\"]\n",
    "    coords = proc.get_coordinates(\n",
    "        data_shape=features_shape,\n",
    "        patch_shape=cfg.data.patch_shape,\n",
    "        split=cfg.data.coord_split,\n",
    "        normalize_range=cfg.data.coord_normalize_range,\n",
    "    )\n",
    "    coords = coords.to(x).cuda()\n",
    "\n",
    "    inference_results = {}\n",
    "    with torch.no_grad():\n",
    "        out = ffn_model(coords, img=data)\n",
    "        pred = out[\"predicted\"]\n",
    "        intermediate_results = out[\"intermediate_results\"]\n",
    "\n",
    "        if reshape:\n",
    "            # This reshapes the prediction into an image\n",
    "            pred = proc.process_outputs(\n",
    "                pred,\n",
    "                input_img_shape=batch[\"data_shape\"].squeeze().tolist(),\n",
    "                features_shape=features_shape,\n",
    "                patch_shape=cfg.data.patch_shape,\n",
    "            )\n",
    "\n",
    "    inference_results = {\n",
    "        \"data\": batch[\"data\"],\n",
    "        \"pred\": pred,\n",
    "        \"annotations\": annotations,\n",
    "        \"img_hw\": (H, W),\n",
    "        \"intermediate_results\": intermediate_results,\n",
    "    }\n",
    "\n",
    "    categories_in_frame = {}\n",
    "    for ann in annotations:\n",
    "        if ann[\"category_id\"] not in categories_in_frame:\n",
    "            categories_in_frame[ann[\"category_id\"]] = categories_dict[\n",
    "                ann[\"category_id\"]\n",
    "            ]\n",
    "\n",
    "    categories_in_frame[-1] = categories_dict[-1]\n",
    "    object_categories = [v[\"name\"] for k, v in categories_in_frame.items()]\n",
    "    categories_in_frame = [v for k, v in categories_in_frame.items()]\n",
    "\n",
    "    return inference_results, categories_in_frame, object_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a57a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_info(inference_results, object_categories, categories):\n",
    "\n",
    "    # Create a map from unique inst_id to a suffix that denotes an instance\n",
    "    # number in current video. Also stores object category.\n",
    "    inst_id_to_cat_and_inst_suffix = {}\n",
    "    object_to_instances_map = {}\n",
    "    obj_to_obj_name_idx = {}\n",
    "    instance_names = []\n",
    "    object_to_instances_map = defaultdict(list)\n",
    "\n",
    "    for idx, object_cat in enumerate(object_categories):\n",
    "        obj_to_obj_name_idx[object_cat] = idx\n",
    "\n",
    "    instance_to_ann_id_map = {}\n",
    "\n",
    "    frame_annos = inference_results[\"annotations\"]\n",
    "    for ann in frame_annos:\n",
    "        category_name = [\n",
    "            cat[\"name\"] for cat in categories if cat[\"id\"] == ann[\"category_id\"]\n",
    "        ][0]\n",
    "        num_instances_of_obj = len(object_to_instances_map[category_name])\n",
    "\n",
    "        if ann[\"inst_id\"] not in list(inst_id_to_cat_and_inst_suffix.keys()):\n",
    "            inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]] = {\n",
    "                \"category\": category_name,\n",
    "                \"inst_suffix\": num_instances_of_obj,\n",
    "                \"instance_name\": category_name + \"_\" + str(num_instances_of_obj),\n",
    "            }\n",
    "\n",
    "        instance_name = inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]][\"instance_name\"]\n",
    "        instance_to_ann_id_map[instance_name] = ann[\"id\"]\n",
    "        if instance_name not in instance_names:\n",
    "            object_to_instances_map[category_name].append(instance_name)\n",
    "            instance_names.append(instance_name)\n",
    "\n",
    "    def custom_sort_key(item):\n",
    "        parts = item.split(\"_\")\n",
    "        return (\"_\".join(parts[:-1]), int(parts[-1]))\n",
    "\n",
    "    instance_names = [item for item in sorted(instance_names, key=custom_sort_key)]\n",
    "    instance_names.append(instance_names.pop(instance_names.index(\"other_0\")))\n",
    "\n",
    "    return (\n",
    "        inst_id_to_cat_and_inst_suffix,\n",
    "        instance_to_ann_id_map,\n",
    "        instance_names,\n",
    "        object_to_instances_map,\n",
    "        obj_to_obj_name_idx,\n",
    "        instance_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffc28a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_contribs(\n",
    "    layer_1_output_contrib,\n",
    "    layer_2_output_contrib,\n",
    "    layer_3_output_contrib,\n",
    "    annotations,\n",
    "    instance_to_ann_id_map,\n",
    "    instance_names,\n",
    "):\n",
    "\n",
    "    # Maps for kernel to object contributions\n",
    "    num_layer_1_weights = layer_1_output_contrib.shape[0]\n",
    "    num_layer_2_weights = layer_2_output_contrib.shape[0]\n",
    "    num_layer_3_weights = layer_3_output_contrib.shape[0]\n",
    "\n",
    "    num_instances = len(instance_names)\n",
    "    layer_1_to_instance_contribs = torch.zeros((num_layer_1_weights, num_instances))\n",
    "    layer_2_to_instance_contribs = torch.zeros((num_layer_2_weights, num_instances))\n",
    "    layer_3_to_instance_contribs = torch.zeros((num_layer_3_weights, num_instances))\n",
    "\n",
    "    for instance in instance_to_ann_id_map:\n",
    "        ann_id = instance_to_ann_id_map[instance]\n",
    "        ann = [ann for ann in annotations if ann[\"id\"] == ann_id][0]\n",
    "\n",
    "        area = ann[\"area\"]\n",
    "        binary_mask = ann[\"binary_mask\"].squeeze()\n",
    "\n",
    "        curr_instance_layer_1_contribs = torch.abs(\n",
    "            layer_1_output_contrib[:, binary_mask]\n",
    "        )\n",
    "        curr_instance_layer_2_contribs = torch.abs(\n",
    "            layer_2_output_contrib[:, binary_mask]\n",
    "        )\n",
    "        curr_instance_layer_3_contribs = torch.abs(\n",
    "            layer_3_output_contrib[:, binary_mask]\n",
    "        )\n",
    "\n",
    "        # Get aggregated total contribution for each kernel to the instance\n",
    "        total_layer_1_contrib = torch.sum(curr_instance_layer_1_contribs, dim=-1)\n",
    "        total_layer_2_contrib = torch.sum(curr_instance_layer_2_contribs, dim=-1)\n",
    "        total_layer_3_contrib = torch.sum(curr_instance_layer_3_contribs, dim=-1)\n",
    "        avg_layer_1_contrib = total_layer_1_contrib / area\n",
    "        avg_layer_2_contrib = total_layer_2_contrib / area\n",
    "        avg_layer_3_contrib = total_layer_3_contrib / area\n",
    "\n",
    "        inst_idx = instance_names.index(instance)\n",
    "        layer_1_to_instance_contribs[:, inst_idx] = avg_layer_1_contrib.flatten()\n",
    "        layer_2_to_instance_contribs[:, inst_idx] = avg_layer_2_contrib.flatten()\n",
    "        layer_3_to_instance_contribs[:, inst_idx] = avg_layer_3_contrib.flatten()\n",
    "\n",
    "    return layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1c820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_contribs(\n",
    "    layer_1_to_instance_contribs,\n",
    "    layer_2_to_instance_contribs,\n",
    "    layer_3_to_instance_contribs,\n",
    "):\n",
    "\n",
    "    layer_1_nonzero_rows = (torch.sum(layer_1_to_instance_contribs, dim=1)) != 0\n",
    "    layer_2_nonzero_rows = (torch.sum(layer_2_to_instance_contribs, dim=1)) != 0\n",
    "    layer_3_nonzero_rows = (torch.sum(layer_3_to_instance_contribs, dim=1)) != 0\n",
    "\n",
    "    # Remove the rows (kernels) whose contributions sum to zeros\n",
    "    layer_1_to_instance_contribs = layer_1_to_instance_contribs[layer_1_nonzero_rows, :]\n",
    "    layer_2_to_instance_contribs = layer_2_to_instance_contribs[layer_2_nonzero_rows, :]\n",
    "    layer_3_to_instance_contribs = layer_3_to_instance_contribs[layer_3_nonzero_rows, :]\n",
    "    layer_1_contribs_normalized_by_instance = (\n",
    "        layer_1_to_instance_contribs\n",
    "        / torch.sum(layer_1_to_instance_contribs, dim=1)[:, None]\n",
    "    )\n",
    "    layer_2_contribs_normalized_by_instance = (\n",
    "        layer_2_to_instance_contribs\n",
    "        / torch.sum(layer_2_to_instance_contribs, dim=1)[:, None]\n",
    "    )\n",
    "    layer_3_contribs_normalized_by_instance = (\n",
    "        layer_3_to_instance_contribs\n",
    "        / torch.sum(layer_3_to_instance_contribs, dim=1)[:, None]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        layer_1_contribs_normalized_by_instance,\n",
    "        layer_2_contribs_normalized_by_instance,\n",
    "        layer_3_contribs_normalized_by_instance,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d4fdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_variables_for_frame(\n",
    "    inference_results, instance_to_ann_id_map, instance_names, ffn_model\n",
    "):\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    (H, W) = inference_results[\"img_hw\"]\n",
    "    annotations = inference_results[\"annotations\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeMLPContributions(\n",
    "        ffn_model, intermediate_results, (H, W)\n",
    "    )\n",
    "\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, _, _, _ = (\n",
    "        compute_contrib_obj.compute_all_layer_mappings()\n",
    "    )\n",
    "    (\n",
    "        layer_1_to_instance_contribs,\n",
    "        layer_2_to_instance_contribs,\n",
    "        layer_3_to_instance_contribs,\n",
    "    ) = get_instance_contribs(\n",
    "        layer_1_output_contrib,\n",
    "        layer_2_output_contrib,\n",
    "        layer_3_output_contrib,\n",
    "        annotations,\n",
    "        instance_to_ann_id_map,\n",
    "        instance_names,\n",
    "    )\n",
    "\n",
    "    # Some of the neurons in MLP are dead (all 0 contribs). These are removed in normalization\n",
    "    (\n",
    "        layer_1_contribs_normalized_by_instance,\n",
    "        layer_2_contribs_normalized_by_instance,\n",
    "        layer_3_contribs_normalized_by_instance,\n",
    "    ) = get_normalized_contribs(\n",
    "        layer_1_to_instance_contribs,\n",
    "        layer_2_to_instance_contribs,\n",
    "        layer_3_to_instance_contribs,\n",
    "    )\n",
    "\n",
    "    all_variables_for_frame = {\n",
    "        \"instance_names\": instance_names,\n",
    "        \"layer_1_contribs_normalized_by_instance\": layer_1_contribs_normalized_by_instance,\n",
    "        \"layer_2_contribs_normalized_by_instance\": layer_2_contribs_normalized_by_instance,\n",
    "        \"layer_3_contribs_normalized_by_instance\": layer_3_contribs_normalized_by_instance,\n",
    "    }\n",
    "\n",
    "    return all_variables_for_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8ea38",
   "metadata": {},
   "source": [
    "## MLP Contributions to Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9faaf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#843c39\",\n",
    "    \"#7b4173\",\n",
    "    \"#5254a3\",\n",
    "    \"#fdc7c7\",\n",
    "    \"#637939\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "    \"#b1c2b3\",\n",
    "    \"#ff9896\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#f5c651\",\n",
    "    \"#02a3a3\",\n",
    "]\n",
    "\n",
    "def plot_kernel_instance_contribs(all_variables_for_frame, obj_to_obj_name_idx, object_categories, custom_colors):\n",
    "    num_instances_per_obj = 3\n",
    "    num_neuron_samples_per_layer = 3\n",
    "\n",
    "    layer_1_contribs_normalized_by_instance = all_variables_for_frame[\"layer_1_contribs_normalized_by_instance\"]\n",
    "    layer_2_contribs_normalized_by_instance = all_variables_for_frame[\"layer_2_contribs_normalized_by_instance\"]\n",
    "    layer_3_contribs_normalized_by_instance = all_variables_for_frame[\"layer_3_contribs_normalized_by_instance\"]\n",
    "    sampled_layer_1_neurons = torch.randperm(layer_1_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "    sampled_layer_2_neurons = torch.randperm(layer_2_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "    sampled_layer_3_neurons = torch.randperm(layer_3_contribs_normalized_by_instance.shape[0])[:num_neuron_samples_per_layer]\n",
    "\n",
    "    layer_to_sampled_neurons = {\n",
    "        1: sampled_layer_1_neurons,\n",
    "        2: sampled_layer_2_neurons,\n",
    "        3: sampled_layer_3_neurons\n",
    "    }\n",
    "\n",
    "    object_to_color_map = dict(zip(object_categories, custom_colors[:len(object_categories)]))\n",
    "    markers = [\".\", \"*\", \"p\", \"X\", \"^\", \"s\", \"2\", \"d\"]\n",
    "    instance_names = all_variables_for_frame[\"instance_names\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,4), tight_layout=True)\n",
    "    x = np.arange(num_neuron_samples_per_layer * 3)\n",
    "    x_offsets = np.linspace(-0.25, 0.25, len(obj_to_obj_name_idx.keys()))\n",
    "\n",
    "    # Get contributions of each instance from each sampled kernel\n",
    "    inst_kernel_contribs = torch.zeros((len(instance_names), num_neuron_samples_per_layer*3))\n",
    "\n",
    "    # Plot neurons for each layer\n",
    "    for layer_idx, layer in enumerate([1, 2, 3]):\n",
    "        for neuron_idx, sampled_neuron_idx in enumerate(layer_to_sampled_neurons[layer]):\n",
    "            if layer == 1:\n",
    "                sampled_layer_contribs = layer_1_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "            elif layer == 2:\n",
    "                sampled_layer_contribs = layer_2_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "            elif layer == 3:\n",
    "                sampled_layer_contribs = layer_3_contribs_normalized_by_instance[sampled_neuron_idx, :] # 1 x num_instances\n",
    "                \n",
    "            inst_kernel_contribs[:, (layer_idx-1)*3 + neuron_idx] = sampled_layer_contribs.squeeze()\n",
    " \n",
    "    # Plot each instance, using a different marker for each instance in a category\n",
    "    leading_obj = '_'.join(instance_names[0].split('_')[0:-1])\n",
    "    marker_idx = -1\n",
    "    for inst_num, inst_name in enumerate(instance_names):\n",
    "        # reset marker index for each new object\n",
    "        if('_'.join(inst_name.split('_')[0:-1]) == leading_obj):\n",
    "            marker_idx += 1\n",
    "        else:\n",
    "            marker_idx = 0\n",
    "            leading_obj = '_'.join(inst_name.split('_')[0:-1])\n",
    "        \n",
    "        if marker_idx < num_instances_per_obj:\n",
    "            data_with_nones = np.where(inst_kernel_contribs[inst_num, :] == 0.0, None, inst_kernel_contribs[inst_num, :]\n",
    "            offset_for_obj = x_offsets[list(obj_to_obj_name_idx.keys()).index(leading_obj)]\n",
    "            label = inst_name.replace(' ', '_')\n",
    "            label = label.replace('vegitation', 'vegetation')\n",
    "            ax.plot(x + offset_for_obj, data_with_nones, marker=markers[marker_idx], linestyle='None', label=label, color=object_to_color_map[leading_obj])\n",
    "            \n",
    "            \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['a', 'b', 'c', 'p', 'q', 'r', 'x', 'y', 'z', ])\n",
    "    ax.set_ylabel(f\"Contributions\")\n",
    "    ax.set_xlabel(f\"Sampled Neurons from Layers 1, 2, 3\")\n",
    "    ax.axvline(x=2.5, linestyle='--', color='gray', alpha=0.4, linewidth=1)\n",
    "    ax.axvline(x=5.5, linestyle='--', color='gray', alpha=0.4, linewidth=1)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=6, bbox_to_anchor=(0.5, 1.10), fontsize='small')\n",
    "    fig.suptitle(f\"MLP Layers - Instance Contributions for Sampled Neurons\", y=1.15)\n",
    "\n",
    "    neuron_to_inst_contrib_dict = {\n",
    "        \"instance_names\": instance_names,\n",
    "        \"object_categories\": object_categories,\n",
    "        \"obj_to_obj_name_idx\": obj_to_obj_name_idx,\n",
    "        \"num_instances_per_obj\": num_instances_per_obj,\n",
    "        \"num_neuron_samples_per_layer\": num_neuron_samples_per_layer,\n",
    "        \"inst_kernel_contribs\": inst_kernel_contribs,\n",
    "    }\n",
    "    return neuron_to_inst_contrib_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9efda",
   "metadata": {},
   "source": [
    "Analyze each image and save the raw values for downstream visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_neuron_to_inst_contrib_dict = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = dataloader_dict[dataset_name][vidname]\n",
    "        ffn_model = ffn_models_dict[dataset_name][vidname]\n",
    "        categories_dict = categories_dicts[dataset_name][vidname]\n",
    "        cfg = cfg_dict[dataset_name][vidname]\n",
    "\n",
    "        categories = list(categories_dict.values())\n",
    "\n",
    "        inference_results, categories_in_frame, object_categories = (\n",
    "            compute_inference_results(\n",
    "                single_image_dataloader, ffn_model, cfg, categories_dict\n",
    "            )\n",
    "        )\n",
    "        (\n",
    "            inst_id_to_cat_and_inst_suffix,\n",
    "            instance_to_ann_id_map,\n",
    "            instance_names,\n",
    "            object_to_instances_map,\n",
    "            obj_to_obj_name_idx,\n",
    "            instance_names,\n",
    "        ) = get_instance_info(inference_results, object_categories, categories)\n",
    "\n",
    "        all_variables_for_frame = compute_all_variables_for_frame(\n",
    "            inference_results, instance_to_ann_id_map, instance_names, ffn_model\n",
    "        )\n",
    "        per_vid_neuron_to_inst_contrib_dict[vidname] = plot_kernel_instance_contribs(\n",
    "            all_variables_for_frame, obj_to_obj_name_idx, object_categories, custom_colors\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311b06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = \"../analysis_data/MLP/contributions_to_objects\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(\n",
    "    os.path.join(save_dir, f\"per_vid_neuron_to_inst_contrib_dict.pkl\"), \"wb\"\n",
    ") as f:\n",
    "    pickle.dump(per_vid_neuron_to_inst_contrib_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
