{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e178c98-4b46-4238-800f-a0e3538f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.filters import gabor_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import data_process, helper\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.append('../')\n",
    "\n",
    "from get_mlp_mappings import ComputeMLPContributions\n",
    "from image_vps_datasets import (single_image_cityscape_vps_dataset,\n",
    "                                single_image_vipseg_dataset)\n",
    "from model_all_analysis import ffn, lightning_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33e00bff-f0ba-4d87-93b9-4fda3ffa418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_annotations_to_numpy(tensor_annotations):\n",
    "    annotations = []\n",
    "    \n",
    "    for tensor_anno in tensor_annotations:\n",
    "        annotation = {}\n",
    "\n",
    "        annotation['id'] = tensor_anno['id'].item()\n",
    "        annotation['inst_id'] = tensor_anno['inst_id'].item()\n",
    "        annotation['image_id'] = tensor_anno['image_id'][0] #.item()\n",
    "        annotation['category_id'] = tensor_anno['category_id'].item()\n",
    "        annotation['area'] = tensor_anno['area'].item()\n",
    "        annotation['iscrowd'] = tensor_anno['iscrowd'].item()\n",
    "        annotation['isthing'] = tensor_anno['isthing'].item()\n",
    "\n",
    "        # Convert 'bbox' back to regular format\n",
    "        bbox = [bbox_tensor.item() for bbox_tensor in tensor_anno['bbox']]\n",
    "        annotation['bbox'] = bbox\n",
    "\n",
    "        annotation['binary_mask'] = tensor_anno['binary_mask'].numpy()\n",
    "        \n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def add_other_annotation(annotations):\n",
    "\n",
    "    # Create a mask that will indicate whether a location contains at least one instance\n",
    "    object_region_mask = None\n",
    "    for ann in annotations:\n",
    "        binary_mask = ann['binary_mask'].squeeze()\n",
    "        if object_region_mask is None:\n",
    "            # If the object_region_mask is None, initialize it to current binary_mask otherwise aggregate it\n",
    "            object_region_mask = binary_mask.copy()\n",
    "        else:\n",
    "            object_region_mask += binary_mask\n",
    "\n",
    "    # Binarize\n",
    "    object_region_mask = object_region_mask != 0\n",
    "    \n",
    "    # Create an annotation denoting \"other\" for regions that have no objects\n",
    "    annotations.append({\n",
    "        \"id\": -1,\n",
    "        \"inst_id\": -1,\n",
    "        \"bbox\": compute_bbox(object_region_mask),\n",
    "        \"area\": object_region_mask.sum(),\n",
    "        \"binary_mask\": object_region_mask,\n",
    "        'iscrowd': 0,\n",
    "        'isthing': 0,\n",
    "        'category_id': -1,\n",
    "        'image_id': annotations[0]['image_id']\n",
    "    })\n",
    "    return annotations\n",
    "\n",
    "def plot_image_with_instances(image, annotations, categories_dict, title=None):\n",
    "    plt.rcParams[\"figure.figsize\"] = 15, 10\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for anno in annotations:\n",
    "        # Skip plotting \"other\" regions (regions without objects)\n",
    "        if anno[\"category_id\"] == -1:\n",
    "            continue\n",
    "        # Draw bbox\n",
    "        x, y, w, h = anno[\"bbox\"]\n",
    "\n",
    "        cat_color = np.array(categories_dict[int(anno[\"category_id\"])]['color']) / 255\n",
    "        rectangle = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=cat_color, facecolor='none')\n",
    "        ax.add_patch(rectangle)\n",
    "\n",
    "        if 'binary_mask' in anno.keys():\n",
    "            binary_mask = anno[\"binary_mask\"].squeeze(0)\n",
    "        else:\n",
    "            raise ValueError(\"No binary mask found in annotation\")\n",
    "        # Create a mask where the binary mask is not zero\n",
    "        mask = binary_mask != 0\n",
    "\n",
    "        # Create rgba mask\n",
    "        cmap = ListedColormap(cat_color)\n",
    "        colored_mask = cmap(binary_mask.astype(float) / 1.0)\n",
    "\n",
    "        # Create a mask where the binary mask is not zero\n",
    "        mask = binary_mask != 0\n",
    "\n",
    "        # Set the alpha channel to 0 for regions where the binary mask is zero\n",
    "        colored_mask[:, :, 3] = mask.astype(float)\n",
    "        \n",
    "        # Display the colored mask over the image\n",
    "        ax.imshow(colored_mask, alpha=0.5)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_bbox(binary_mask):\n",
    "    (rows, cols) = np.where(binary_mask > 0)\n",
    "    x_min, x_max, y_min, y_max = min(cols), max(cols), min(rows), max(rows)\n",
    "    # Create the bbox in COCO format [x, y, width, height]\n",
    "    width = x_max - x_min + 1\n",
    "    height = y_max - y_min + 1\n",
    "    bbox = [x_min, y_min, width, height]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83ec5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfg(model_ckpt_dir, dataset_name, vidname):\n",
    "    \n",
    "    if dataset_name == \"cityscapes\":\n",
    "        # Add cityscapes VPS paths\n",
    "        exp_config_path = os.path.join(model_ckpt_dir, 'exp_config.yaml')\n",
    "        \n",
    "        cfg = OmegaConf.load(exp_config_path)\n",
    "        \n",
    "        cfg.data.cityscapes_vps_root = \"../data/cityscapes_vps\"\n",
    "        cfg.data.split = \"val\"\n",
    "        cfg.data.panoptic_video_mask_dir = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"panoptic_video\")\n",
    "        cfg.data.panoptic_inst_mask_dir = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"panoptic_inst\")\n",
    "        \n",
    "        cfg.data.vidname = vidname\n",
    "        # We will work with the first annotated frame in the given video\n",
    "        cfg.data.frame_num_in_video = 0\n",
    "        \n",
    "        cfg.data.data_path = os.path.join(cfg.data.cityscapes_vps_root, cfg.data.split, \"img_all\")\n",
    "        cfg.data.anno_path = '../data/cityscapes_vps/panoptic_gt_val_city_vps.json'\n",
    "        \n",
    "        with open(cfg.data.anno_path, 'r') as f:\n",
    "            panoptic_gt_val_city_vps = json.load(f)\n",
    "                    \n",
    "        panoptic_categories = panoptic_gt_val_city_vps['categories']\n",
    "        \n",
    "        categories = panoptic_categories\n",
    "        categories.append(\n",
    "            {'id': -1, 'name': 'other', 'supercategory': '', 'color':None}\n",
    "        )\n",
    "        categories_dict = {el['id']: el for el in categories}\n",
    "\n",
    "    elif dataset_name == \"vipseg\":\n",
    "\n",
    "        exp_config_path = os.path.join(model_ckpt_dir, 'exp_config.yaml')\n",
    "        \n",
    "        \n",
    "        cfg = OmegaConf.load(exp_config_path)\n",
    "        \n",
    "        cfg.data.VIPSeg_720P_root = '../data/VIPSeg-Dataset/VIPSeg/VIPSeg_720P'\n",
    "        cfg.data.panomasks_dir = os.path.join(cfg.data.VIPSeg_720P_root, \"panomasks\")\n",
    "        cfg.data.panomasksRGB_dir = os.path.join(cfg.data.VIPSeg_720P_root, \"panomasksRGB\")\n",
    "        \n",
    "        cfg.data.vidname = vidname\n",
    "        # We will work with the first annotated frame in the given video\n",
    "        cfg.data.frame_num_in_video = 0\n",
    "        \n",
    "        cfg.data.data_path = data_path = os.path.join(cfg.data.VIPSeg_720P_root, \"images\")\n",
    "        cfg.data.anno_path = '../data/VIPSeg-Dataset/VIPSeg/VIPSeg_720P/panoptic_gt_VIPSeg.json'\n",
    "        \n",
    "        # Crop for VIPSeg to match NeRV\n",
    "        cfg.data.crop=[640,1280]\n",
    "        \n",
    "        with open(cfg.data.anno_path, 'r') as f:\n",
    "            panoptic_gt_VIPSeg = json.load(f)\n",
    "                    \n",
    "        panoptic_categories = panoptic_gt_VIPSeg['categories']\n",
    "        # panoptic_videos = panoptic_gt_VIPSeg['videos']\n",
    "        # panoptic_annotations = panoptic_gt_VIPSeg['annotations']    \n",
    "        \n",
    "        categories = panoptic_categories\n",
    "        categories.append(\n",
    "            {'id': -1, 'name': 'other', 'supercategory': '', 'color':None}\n",
    "        )\n",
    "        categories_dict = {el['id']: el for el in categories}\n",
    "        \n",
    "    return cfg, categories_dict\n",
    "\n",
    "# object_categories = [v['name'] for k, v in categories_dict.items()]\n",
    "\n",
    "\n",
    "def load_model(cfg):\n",
    "    save_dir = cfg.logging.checkpoint.logdir\n",
    "    ckpt_path = helper.find_ckpt(save_dir)\n",
    "    print(f'Loading checkpoint from {ckpt_path}')\n",
    "\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "    # Load checkpoint into this wrapper model cause that is what is stored in disk :)\n",
    "    model = lightning_model(cfg, ffn(cfg))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    ffn_model = model.model\n",
    "    \n",
    "    return ffn_model.cuda()\n",
    "\n",
    "def get_loader(cfg,dataset_name,val=False):\n",
    "    # use the dataloader which returns image along with annotations\n",
    "    if dataset_name == \"cityscapes\":\n",
    "        img_dataset = single_image_cityscape_vps_dataset(cfg)\n",
    "    else:\n",
    "        img_dataset = single_image_vipseg_dataset(cfg)\n",
    "    #create torch dataset for one image.\n",
    "    loader = DataLoader(img_dataset, batch_size=1, shuffle = False ,num_workers=0)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e61d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple videos\n",
    "dataset_names = ['cityscapes', 'vipseg']\n",
    "# Choose videos with 30ish instances at least\n",
    "vidnames = {\n",
    "    'cityscapes': ['0005'],#, '0175'],\n",
    "    'vipseg': ['26_cblDl5vCZnw'] # '12_n-ytHkMceew'\n",
    "}\n",
    "\n",
    "vid_data_folder_name = {\n",
    "    \"cityscapes\": \"Cityscapes_VPS_models\",\n",
    "    \"vipseg\": \"VIPSeg_models\"\n",
    "}\n",
    "\n",
    "\n",
    "cfg_dict = {}\n",
    "dataloader_dict = {}\n",
    "weights_dict = {}\n",
    "ffn_models_dict = {}\n",
    "categories_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d7b8997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from output/Cityscapes_VPS_models/0005/0005_framenum_0_128_256/best_model-v1.ckpt\n",
      "Using loss function :  mse  with weight:  1\n",
      "Loading checkpoint from output/VIPSeg_models/26_cblDl5vCZnw/26_cblDl5vCZnw_framenum_0_128_256/best_model-v1.ckpt\n",
      "Using loss function :  mse  with weight:  1\n",
      "image_id:  0005_0025_frankfurt_000000_001736\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    weights_dict[dataset_name] = {}\n",
    "    cfg_dict[dataset_name] = {}\n",
    "    ffn_models_dict[dataset_name] = {}\n",
    "    categories_dicts[dataset_name] = {}\n",
    "\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        vid_data_folder = vid_data_folder_name[dataset_name]\n",
    "        weights_dict[dataset_name][vidname] = f'output/{vid_data_folder}/{vidname}/{vidname}_framenum_0_128_256'\n",
    "        \n",
    "        cfg, categories_dict = load_cfg(weights_dict[dataset_name][vidname], dataset_name, vidname)\n",
    "        cfg_dict[dataset_name][vidname] = cfg\n",
    "        categories_dicts[dataset_name][vidname] = categories_dict\n",
    "        \n",
    "        \n",
    "        ffn_models_dict[dataset_name][vidname] = load_model(cfg)\n",
    "        \n",
    "for dataset_name in dataset_names:\n",
    "    dataloader_dict[dataset_name] = {}\n",
    "    \n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = get_loader(cfg_dict[dataset_name][vidname], dataset_name)\n",
    "        \n",
    "        dataloader_dict[dataset_name][vidname] = single_image_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a57a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_info(inference_results, object_categories, categories):\n",
    "    \n",
    "    # Create a map from unique inst_id to a suffix that denotes an instance number in current video. Also stores object category.\n",
    "    inst_id_to_cat_and_inst_suffix = {}\n",
    "    \n",
    "    object_to_instances_map = {}\n",
    "    obj_to_obj_name_idx = {}\n",
    "    \n",
    "    instance_names = []\n",
    "    object_to_instances_map = defaultdict(list)\n",
    "    \n",
    "    for idx, object_cat in enumerate(object_categories):\n",
    "        obj_to_obj_name_idx[object_cat] = idx\n",
    "    \n",
    "    instance_to_ann_id_map = {}\n",
    "\n",
    "    # Get annos for current frame\n",
    "    frame_annos = inference_results[\"annotations\"]\n",
    "    for ann in frame_annos:\n",
    "        category_name = [cat[\"name\"] for cat in categories if cat[\"id\"] == ann[\"category_id\"]][0]\n",
    "        \n",
    "        # Get the current number of instances of this category            \n",
    "        num_instances_of_obj = len(object_to_instances_map[category_name])\n",
    "        \n",
    "        if ann[\"inst_id\"] not in list(inst_id_to_cat_and_inst_suffix.keys()):\n",
    "            # Create a dictionary for the instance\n",
    "            inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]] = {\n",
    "                \"category\": category_name,\n",
    "                \"inst_suffix\": num_instances_of_obj, #0\n",
    "                \"instance_name\": category_name + '_' + str(num_instances_of_obj)\n",
    "            }\n",
    "\n",
    "        # Retrieve the stored instance name\n",
    "        instance_name = inst_id_to_cat_and_inst_suffix[ann[\"inst_id\"]][\"instance_name\"]\n",
    "\n",
    "        instance_to_ann_id_map[instance_name] = ann['id']\n",
    "\n",
    "        if instance_name not in instance_names:\n",
    "            object_to_instances_map[category_name].append(instance_name)\n",
    "            instance_names.append(instance_name)\n",
    "\n",
    "    def custom_sort_key(item):\n",
    "        parts = item.split('_')\n",
    "        return (\"_\".join(parts[:-1]), int(parts[-1]))\n",
    "        \n",
    "    # Sort the instance names\n",
    "    instance_names = [item for item in sorted(instance_names, key=custom_sort_key)]\n",
    "    \n",
    "    # Find \"other_0\" instance in this list and move it to the back\n",
    "    instance_names.append(instance_names.pop(instance_names.index(\"other_0\")))\n",
    "    \n",
    "    return inst_id_to_cat_and_inst_suffix, instance_to_ann_id_map, instance_names, object_to_instances_map, obj_to_obj_name_idx, instance_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffc28a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each instance - get average contrib, total contrib and total area (other useful info too)\n",
    "def get_instance_contribs(\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, annotations, instance_to_ann_id_map, instance_names \n",
    "):\n",
    "    total_img_area = layer_1_output_contrib.size(-2) * layer_1_output_contrib.size(-1)\n",
    "    \n",
    "    # Maps for kernel to object contributions\n",
    "    num_layer_1_neurons = layer_1_output_contrib.shape[0]\n",
    "    num_layer_2_neurons = layer_2_output_contrib.shape[0]\n",
    "    num_layer_3_neurons = layer_3_output_contrib.shape[0]\n",
    "\n",
    "    num_instances = len(instance_names)\n",
    "    layer_1_to_instance_contribs = torch.zeros((num_layer_1_neurons, num_instances))\n",
    "    layer_2_to_instance_contribs = torch.zeros((num_layer_2_neurons, num_instances))\n",
    "    layer_3_to_instance_contribs = torch.zeros((num_layer_3_neurons, num_instances))\n",
    "\n",
    "    instance_areas = torch.zeros(num_instances)\n",
    "    \n",
    "    # Use deltas idea to find percentage deviation of instance's actual contribution from expected\n",
    "    layer_1_instance_contrib_ratio_to_total = torch.zeros((num_layer_1_neurons, num_instances))\n",
    "    layer_2_instance_contrib_ratio_to_total = torch.zeros((num_layer_2_neurons, num_instances))\n",
    "    layer_3_instance_contrib_ratio_to_total = torch.zeros((num_layer_3_neurons, num_instances))\n",
    "    \n",
    "    # Store the total neuron-wise contributions to output image\n",
    "    total_layer_1_output_contrib = torch.sum(torch.abs(layer_1_output_contrib), dim=(1,2))\n",
    "    total_layer_2_output_contrib = torch.sum(torch.abs(layer_2_output_contrib), dim=(1,2))\n",
    "    total_layer_3_output_contrib = torch.sum(torch.abs(layer_3_output_contrib), dim=(1,2))\n",
    "\n",
    "    for instance in instance_to_ann_id_map:\n",
    "        ann_id = instance_to_ann_id_map[instance]\n",
    "        ann = [ann for ann in annotations if ann['id'] == ann_id][0]\n",
    "        \n",
    "        area = ann['area']\n",
    "        binary_mask = ann['binary_mask'].squeeze()\n",
    "        \n",
    "        # Use binary mask of shape hxw to index into the n_featsxhxw contribution tensor\n",
    "        # to get the contribs for the current instance\n",
    "        curr_instance_layer_1_contribs = torch.abs(layer_1_output_contrib[:, binary_mask])\n",
    "        curr_instance_layer_2_contribs = torch.abs(layer_2_output_contrib[:, binary_mask])\n",
    "        curr_instance_layer_3_contribs = torch.abs(layer_3_output_contrib[:, binary_mask])\n",
    "        \n",
    "        # Get aggregated total contribution for each kernel to the instance\n",
    "        total_layer_1_inst_contrib = torch.sum(curr_instance_layer_1_contribs, dim=-1)\n",
    "        total_layer_2_inst_contrib = torch.sum(curr_instance_layer_2_contribs, dim=-1)\n",
    "        total_layer_3_inst_contrib = torch.sum(curr_instance_layer_3_contribs, dim=-1)\n",
    "        avg_layer_1_contrib = total_layer_1_inst_contrib / area\n",
    "        avg_layer_2_contrib = total_layer_2_inst_contrib / area\n",
    "        avg_layer_3_contrib = total_layer_3_inst_contrib / area\n",
    "            \n",
    "        # Store the average contribution from each layer neurons to current instance\n",
    "        inst_idx = instance_names.index(instance)\n",
    "        layer_1_to_instance_contribs[:, inst_idx] = avg_layer_1_contrib.flatten()\n",
    "        layer_2_to_instance_contribs[:, inst_idx] = avg_layer_2_contrib.flatten()\n",
    "        layer_3_to_instance_contribs[:, inst_idx] = avg_layer_3_contrib.flatten()\n",
    "        \n",
    "        # Find delta percentages - ( true contrib - expected contrib ) / expected contrib\n",
    "        layer_1_expected_instance_contrib = total_layer_1_output_contrib * (area / total_img_area)\n",
    "        layer_1_instance_contrib_ratio_to_total[:, inst_idx] = torch.abs(total_layer_1_inst_contrib - layer_1_expected_instance_contrib) / layer_1_expected_instance_contrib\n",
    "\n",
    "        layer_2_expected_instance_contrib = total_layer_2_output_contrib * (area / total_img_area)\n",
    "        layer_2_instance_contrib_ratio_to_total[:, inst_idx] = torch.abs(total_layer_2_inst_contrib - layer_2_expected_instance_contrib) / layer_2_expected_instance_contrib\n",
    "\n",
    "        layer_3_expected_instance_contrib = total_layer_3_output_contrib * (area / total_img_area)\n",
    "        layer_3_instance_contrib_ratio_to_total[:, inst_idx] = torch.abs(total_layer_3_inst_contrib - layer_3_expected_instance_contrib) / layer_3_expected_instance_contrib\n",
    "\n",
    "    return layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs, \\\n",
    "        layer_1_instance_contrib_ratio_to_total, layer_2_instance_contrib_ratio_to_total, layer_3_instance_contrib_ratio_to_total, instance_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d464f2c-1e3b-45cc-8379-ac823bdbbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridcell_contribs(\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, reg_stride_h, reg_stride_w\n",
    "):\n",
    "    total_img_area = layer_1_output_contrib.size(-2) * layer_1_output_contrib.size(-1)\n",
    "    \n",
    "    # Take absolute of contributions # num_neurons x cell_stride x cell_stride x h/cell_stride x w/cell_stride. e.g. k x 4 x 4 x h/4 x w/4\n",
    "    unfolded_layer_1_to_gridcell_contribs = torch.abs(layer_1_output_contrib).unfold(1, reg_stride_h, reg_stride_h).unfold(2, reg_stride_w, reg_stride_w).permute(0, 3, 4, 1, 2)\n",
    "    unfolded_layer_2_to_gridcell_contribs = torch.abs(layer_2_output_contrib).unfold(1, reg_stride_h, reg_stride_h).unfold(2, reg_stride_w, reg_stride_w).permute(0, 3, 4, 1, 2)\n",
    "    unfolded_layer_3_to_gridcell_contribs = torch.abs(layer_3_output_contrib).unfold(1, reg_stride_h, reg_stride_h).unfold(2, reg_stride_w, reg_stride_w).permute(0, 3, 4, 1, 2)\n",
    "\n",
    "    # Store the total neuron-wise contributions to output image\n",
    "    total_layer_1_output_contrib = torch.sum(torch.abs(layer_1_output_contrib), dim=(1,2))\n",
    "    total_layer_2_output_contrib = torch.sum(torch.abs(layer_2_output_contrib), dim=(1,2))\n",
    "    total_layer_3_output_contrib = torch.sum(torch.abs(layer_3_output_contrib), dim=(1,2))\n",
    "\n",
    "    gridcell_area = unfolded_layer_1_to_gridcell_contribs.size(3) * unfolded_layer_1_to_gridcell_contribs.size(4)\n",
    "    \n",
    "    # take absolute of contributions **after** we store our raw per-region contribs\n",
    "    layer_1_to_gridcell_contribs = torch.abs(unfolded_layer_1_to_gridcell_contribs)\n",
    "    layer_2_to_gridcell_contribs = torch.abs(unfolded_layer_2_to_gridcell_contribs)\n",
    "    layer_3_to_gridcell_contribs = torch.abs(unfolded_layer_3_to_gridcell_contribs)\n",
    "        \n",
    "    # Flatten contribs by region before taking variance over pixels in region\n",
    "    flattened_layer_1_gridcell_contribs = layer_1_to_gridcell_contribs.flatten(3, 4).flatten(1, 2) # num_neurons x num_gridcells x h/cell_stride*w/cell_stride\n",
    "    flattened_layer_2_gridcell_contribs = layer_2_to_gridcell_contribs.flatten(3, 4).flatten(1, 2)\n",
    "    flattened_layer_3_gridcell_contribs = layer_3_to_gridcell_contribs.flatten(3, 4).flatten(1, 2)\n",
    "\n",
    "\n",
    "    # Find delta percentages - ( true contrib - expected contrib ) / expected contrib\n",
    "    layer_1_expected_region_contrib = total_layer_1_output_contrib[:,None] * (gridcell_area / total_img_area)\n",
    "    layer_1_gridcell_contrib_ratio_to_total = (torch.sum(flattened_layer_1_gridcell_contribs, dim=-1) - layer_1_expected_region_contrib) / layer_1_expected_region_contrib\n",
    "    \n",
    "    layer_2_expected_region_contrib = total_layer_2_output_contrib[:,None] * (gridcell_area / total_img_area)\n",
    "    layer_2_gridcell_contrib_ratio_to_total = (torch.sum(flattened_layer_2_gridcell_contribs, dim=-1) - layer_2_expected_region_contrib) / layer_2_expected_region_contrib\n",
    "    \n",
    "    layer_3_expected_region_contrib = total_layer_3_output_contrib[:,None] * (gridcell_area / total_img_area)\n",
    "    layer_3_gridcell_contrib_ratio_to_total = (torch.sum(flattened_layer_3_gridcell_contribs, dim=-1) - layer_3_expected_region_contrib) / layer_3_expected_region_contrib\n",
    "\n",
    "    # Aggregate the maps by summing up contributions within each cell_stride x cell_stride region of size h/cell_stride and w/cell_stride    \n",
    "    # num_neurons x cell_stride x cell_stride\n",
    "    layer_1_to_gridcell_contribs = layer_1_to_gridcell_contribs.sum(dim=(3, 4)) / gridcell_area\n",
    "    layer_2_to_gridcell_contribs = layer_2_to_gridcell_contribs.sum(dim=(3, 4)) / gridcell_area\n",
    "    layer_3_to_gridcell_contribs = layer_3_to_gridcell_contribs.sum(dim=(3, 4)) / gridcell_area\n",
    "\n",
    "    # Reshape the (cell_stride x cell_stride) dim to num_gridcells\n",
    "    layer_1_feature_vectors = layer_1_to_gridcell_contribs.view(layer_1_to_gridcell_contribs.size(0), -1) # num_neurons x num_gridcells\n",
    "    layer_2_feature_vectors = layer_2_to_gridcell_contribs.view(layer_2_to_gridcell_contribs.size(0), -1) # num_neurons x num_gridcells\n",
    "    layer_3_feature_vectors = layer_3_to_gridcell_contribs.view(layer_3_to_gridcell_contribs.size(0), -1) # num_neurons x num_gridcells\n",
    "\n",
    "    return layer_1_feature_vectors, layer_2_feature_vectors, layer_3_feature_vectors, \\\n",
    "        layer_1_gridcell_contrib_ratio_to_total, layer_2_gridcell_contrib_ratio_to_total, layer_3_gridcell_contrib_ratio_to_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b16446f2-dfd4-4677-8014-25efc9068263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kmeans_clusters_in_rgb(image, num_clusters):\n",
    "    # Reshape to 2D array of num_pixels x 3 (for rgb)\n",
    "    image_reshaped_rgb = image.reshape(-1, 3)\n",
    "    \n",
    "    # Perform kmeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_init=1, random_state=0).fit(image_reshaped_rgb)\n",
    "    rgb_cluster_map = kmeans.labels_.reshape(image.shape[0], image.shape[1])\n",
    "    \n",
    "    return rgb_cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04a0ac55-69da-4a09-ad7b-ab02b45827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each rgb cluster - get average contrib, total contrib and total area (other useful info too)\n",
    "def get_rgb_cluster_contribs(\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, rgb_cluster_map \n",
    "):\n",
    "    total_img_area = layer_1_output_contrib.size(-2) * layer_1_output_contrib.size(-1)\n",
    "    \n",
    "    # Maps for kernel to object contributions\n",
    "    num_layer_1_neurons = layer_1_output_contrib.shape[0]\n",
    "    num_layer_2_neurons = layer_2_output_contrib.shape[0]\n",
    "    num_layer_3_neurons = layer_3_output_contrib.shape[0]\n",
    "\n",
    "    n_rgb_clusters = len(np.unique(rgb_cluster_map))\n",
    "    layer_1_to_rgb_cluster_contribs = torch.zeros((num_layer_1_neurons, n_rgb_clusters))\n",
    "    layer_2_to_rgb_cluster_contribs = torch.zeros((num_layer_2_neurons, n_rgb_clusters))\n",
    "    layer_3_to_rgb_cluster_contribs = torch.zeros((num_layer_3_neurons, n_rgb_clusters))\n",
    "\n",
    "    rgb_cluster_areas = torch.zeros(n_rgb_clusters)\n",
    "    \n",
    "    # Use deltas idea to find percentage deviation of rgb cluster's actual contribution from expected\n",
    "    layer_1_rgb_cluster_contrib_ratio_to_total = torch.zeros((num_layer_1_neurons, n_rgb_clusters))\n",
    "    layer_2_rgb_cluster_contrib_ratio_to_total = torch.zeros((num_layer_2_neurons, n_rgb_clusters))\n",
    "    layer_3_rgb_cluster_contrib_ratio_to_total = torch.zeros((num_layer_3_neurons, n_rgb_clusters))\n",
    "    \n",
    "    # Store the total neuron-wise contributions to output image\n",
    "    total_layer_1_output_contrib = torch.sum(torch.abs(layer_1_output_contrib), dim=(1,2))\n",
    "    total_layer_2_output_contrib = torch.sum(torch.abs(layer_2_output_contrib), dim=(1,2))\n",
    "    total_layer_3_output_contrib = torch.sum(torch.abs(layer_3_output_contrib), dim=(1,2))\n",
    "\n",
    "    for cluster_id in np.unique(rgb_cluster_map):\n",
    "        \n",
    "        # Construct a binary mask of shape hxw for the current rgb cluster\n",
    "        binary_mask = (rgb_cluster_map == cluster_id)\n",
    "        binary_mask = binary_mask.squeeze().astype(bool)\n",
    "        area = binary_mask.sum()\n",
    "        \n",
    "        # Use binary mask of shape hxw to index into the n_featsxhxw contribution tensor\n",
    "        # to get the contribs for the current rgb cluster\n",
    "        curr_rgb_cluster_layer_1_contribs = torch.abs(layer_1_output_contrib[:, binary_mask])\n",
    "        curr_rgb_cluster_layer_2_contribs = torch.abs(layer_2_output_contrib[:, binary_mask])\n",
    "        curr_rgb_cluster_layer_3_contribs = torch.abs(layer_3_output_contrib[:, binary_mask])\n",
    "        \n",
    "        # Get aggregated total contribution for each kernel to the superpixel\n",
    "        total_layer_1_spix_contrib = torch.sum(curr_rgb_cluster_layer_1_contribs, dim=-1)\n",
    "        total_layer_2_spix_contrib = torch.sum(curr_rgb_cluster_layer_2_contribs, dim=-1)\n",
    "        total_layer_3_spix_contrib = torch.sum(curr_rgb_cluster_layer_3_contribs, dim=-1)\n",
    "        avg_layer_1_contrib = total_layer_1_spix_contrib / area\n",
    "        avg_layer_2_contrib = total_layer_2_spix_contrib / area\n",
    "        avg_layer_3_contrib = total_layer_3_spix_contrib / area\n",
    "            \n",
    "        # Store the average contribution from each layer neurons to current rgb cluster\n",
    "        layer_1_to_rgb_cluster_contribs[:, cluster_id] = avg_layer_1_contrib.flatten()\n",
    "        layer_2_to_rgb_cluster_contribs[:, cluster_id] = avg_layer_2_contrib.flatten()\n",
    "        layer_3_to_rgb_cluster_contribs[:, cluster_id] = avg_layer_3_contrib.flatten()\n",
    "        \n",
    "        # Find delta percentages -> ( true contrib - expected contrib ) / expected contrib\n",
    "        layer_1_expected_rgb_cluster_contrib = total_layer_1_output_contrib * (area / total_img_area)\n",
    "        layer_1_rgb_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_1_spix_contrib - layer_1_expected_rgb_cluster_contrib) / layer_1_expected_rgb_cluster_contrib\n",
    "\n",
    "        layer_2_expected_rgb_cluster_contrib = total_layer_2_output_contrib * (area / total_img_area)\n",
    "        layer_2_rgb_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_2_spix_contrib - layer_2_expected_rgb_cluster_contrib) / layer_2_expected_rgb_cluster_contrib\n",
    "\n",
    "        layer_3_expected_rgb_cluster_contrib = total_layer_3_output_contrib * (area / total_img_area)\n",
    "        layer_3_rgb_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_3_spix_contrib - layer_3_expected_rgb_cluster_contrib) / layer_3_expected_rgb_cluster_contrib\n",
    "\n",
    "    return layer_1_to_rgb_cluster_contribs, layer_2_to_rgb_cluster_contribs, layer_3_to_rgb_cluster_contribs, \\\n",
    "        layer_1_rgb_cluster_contrib_ratio_to_total, layer_2_rgb_cluster_contrib_ratio_to_total, layer_3_rgb_cluster_contrib_ratio_to_total, rgb_cluster_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2e641af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def get_gabor_kernels():\n",
    "    kernels = []\n",
    "    for theta in range(4):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1, 3):\n",
    "            for frequency in (0.05, 0.25, 0.5):\n",
    "                kernel = np.real(gabor_kernel(frequency, theta=theta,\n",
    "                                            sigma_x=sigma, sigma_y=sigma))\n",
    "                kernels.append(kernel)\n",
    "        return kernels\n",
    "\n",
    "def compute_gabor(image):\n",
    "    kernels = get_gabor_kernels()\n",
    "    all_filtered = []\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')[...,None]\n",
    "        all_filtered.append(filtered)\n",
    "    all_filtered = np.concatenate(all_filtered, axis=-1)\n",
    "    return all_filtered\n",
    "\n",
    "def get_spatial_clustering(img, clustering_type, num_clusters=None):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    img_pixels = rearrange(img, 'h w c -> (h w) c')\n",
    "    if clustering_type == 'kmeans':\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(img_pixels)\n",
    "        labels = kmeans.labels_\n",
    "    else:\n",
    "        raise NotImplementedError(f'{clustering_type} not implemented')\n",
    "    labels = rearrange(labels, '(h w) -> h w', h=img_h, w=img_w)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_gabor_label_map(img, num_clusters):\n",
    "    img_to_cluster = img.copy()\n",
    "    img_to_cluster = np.array(Image.fromarray(img_to_cluster).convert('L'))\n",
    "    img_to_cluster = compute_gabor(img_to_cluster)\n",
    "    label_map = get_spatial_clustering(img_to_cluster, clustering_type='kmeans', num_clusters=num_clusters)\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5398d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each gabor cluster - get average contrib, total contrib and total area (other useful info too)\n",
    "def get_gabor_cluster_contribs(\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, gabor_cluster_map \n",
    "):\n",
    "    total_img_area = layer_1_output_contrib.size(-2) * layer_1_output_contrib.size(-1)\n",
    "    \n",
    "    # Maps for kernel to object contributions\n",
    "    num_layer_1_neurons = layer_1_output_contrib.shape[0]\n",
    "    num_layer_2_neurons = layer_2_output_contrib.shape[0]\n",
    "    num_layer_3_neurons = layer_3_output_contrib.shape[0]\n",
    "\n",
    "    n_gabor_clusters = len(np.unique(gabor_cluster_map))\n",
    "    layer_1_to_gabor_cluster_contribs = torch.zeros((num_layer_1_neurons, n_gabor_clusters))\n",
    "    layer_2_to_gabor_cluster_contribs = torch.zeros((num_layer_2_neurons, n_gabor_clusters))\n",
    "    layer_3_to_gabor_cluster_contribs = torch.zeros((num_layer_3_neurons, n_gabor_clusters))\n",
    "\n",
    "    gabor_cluster_areas = torch.zeros(n_gabor_clusters)\n",
    "    \n",
    "    # Use deltas idea to find percentage deviation of gabor cluster's actual contribution from expected\n",
    "    layer_1_gabor_cluster_contrib_ratio_to_total = torch.zeros((num_layer_1_neurons, n_gabor_clusters))\n",
    "    layer_2_gabor_cluster_contrib_ratio_to_total = torch.zeros((num_layer_2_neurons, n_gabor_clusters))\n",
    "    layer_3_gabor_cluster_contrib_ratio_to_total = torch.zeros((num_layer_3_neurons, n_gabor_clusters))\n",
    "    \n",
    "    # Store the total neuron-wise contributions to output image\n",
    "    total_layer_1_output_contrib = torch.sum(torch.abs(layer_1_output_contrib), dim=(1,2))\n",
    "    total_layer_2_output_contrib = torch.sum(torch.abs(layer_2_output_contrib), dim=(1,2))\n",
    "    total_layer_3_output_contrib = torch.sum(torch.abs(layer_3_output_contrib), dim=(1,2))\n",
    "\n",
    "    for cluster_id in np.unique(gabor_cluster_map):\n",
    "        \n",
    "        # Construct a binary mask of shape hxw for the current rgb cluster\n",
    "        binary_mask = (gabor_cluster_map == cluster_id)\n",
    "        binary_mask = binary_mask.squeeze().astype(bool)\n",
    "        area = binary_mask.sum()\n",
    "        \n",
    "        # Use binary mask of shape hxw to index into the n_featsxhxw contribution tensor\n",
    "        # to get the contribs for the current gabor cluster\n",
    "        curr_gabor_cluster_layer_1_contribs = torch.abs(layer_1_output_contrib[:, binary_mask])\n",
    "        curr_gabor_cluster_layer_2_contribs = torch.abs(layer_2_output_contrib[:, binary_mask])\n",
    "        curr_gabor_cluster_layer_3_contribs = torch.abs(layer_3_output_contrib[:, binary_mask])\n",
    "        \n",
    "        # Get aggregated total contribution for each kernel to the superpixel\n",
    "        total_layer_1_gabor_clust_contrib = torch.sum(curr_gabor_cluster_layer_1_contribs, dim=-1)\n",
    "        total_layer_2_gabor_clust_contrib = torch.sum(curr_gabor_cluster_layer_2_contribs, dim=-1)\n",
    "        total_layer_3_gabor_clust_contrib = torch.sum(curr_gabor_cluster_layer_3_contribs, dim=-1)\n",
    "        avg_layer_1_contrib = total_layer_1_gabor_clust_contrib / area\n",
    "        avg_layer_2_contrib = total_layer_2_gabor_clust_contrib / area\n",
    "        avg_layer_3_contrib = total_layer_3_gabor_clust_contrib / area\n",
    "            \n",
    "        # Store the average contribution from each layer neurons to current gabor cluster\n",
    "        layer_1_to_gabor_cluster_contribs[:, cluster_id] = avg_layer_1_contrib.flatten()\n",
    "        layer_2_to_gabor_cluster_contribs[:, cluster_id] = avg_layer_2_contrib.flatten()\n",
    "        layer_3_to_gabor_cluster_contribs[:, cluster_id] = avg_layer_3_contrib.flatten()\n",
    "        \n",
    "        # Find delta percentages -> ( true contrib - expected contrib ) / expected contrib\n",
    "        layer_1_expected_gabor_cluster_contrib = total_layer_1_output_contrib * (area / total_img_area)\n",
    "        layer_1_gabor_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_1_gabor_clust_contrib - layer_1_expected_gabor_cluster_contrib) / layer_1_expected_gabor_cluster_contrib\n",
    "\n",
    "        layer_2_expected_gabor_cluster_contrib = total_layer_2_output_contrib * (area / total_img_area)\n",
    "        layer_2_gabor_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_2_gabor_clust_contrib - layer_2_expected_gabor_cluster_contrib) / layer_2_expected_gabor_cluster_contrib\n",
    "\n",
    "        layer_3_expected_gabor_cluster_contrib = total_layer_3_output_contrib * (area / total_img_area)\n",
    "        layer_3_gabor_cluster_contrib_ratio_to_total[:, cluster_id] = torch.abs(total_layer_3_gabor_clust_contrib - layer_3_expected_gabor_cluster_contrib) / layer_3_expected_gabor_cluster_contrib\n",
    "\n",
    "    return layer_1_to_gabor_cluster_contribs, layer_2_to_gabor_cluster_contribs, layer_3_to_gabor_cluster_contribs, \\\n",
    "        layer_1_gabor_cluster_contrib_ratio_to_total, layer_2_gabor_cluster_contrib_ratio_to_total, layer_3_gabor_cluster_contrib_ratio_to_total, gabor_cluster_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bb43de8-648e-4961-a723-b8d8bdb94980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_results(single_image_dataloader, ffn_model, cfg, categories_dict, num_rgb_clusters, num_gabor_clusters):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(single_image_dataloader))\n",
    "\n",
    "    data = batch['data'].cuda()\n",
    "    N,C,H,W = data.shape\n",
    "    annotations = convert_tensor_annotations_to_numpy(batch['annotations'])\n",
    "    annotations = add_other_annotation(annotations)\n",
    "\n",
    "    features = batch['features'].squeeze().cuda()\n",
    "    features_shape = batch['features_shape'].squeeze().tolist()\n",
    "    reshape = True\n",
    "\n",
    "    proc = data_process.DataProcessor(cfg.data, device='cpu')\n",
    "    x = batch['data']\n",
    "    coords = proc.get_coordinates(data_shape=features_shape,patch_shape=cfg.data.patch_shape,\\\n",
    "                                    split=cfg.data.coord_split,normalize_range=cfg.data.coord_normalize_range)\n",
    "    coords = coords.to(x).cuda()\n",
    "\n",
    "    # Create a dictionary to store the intermediate decoder_results from each seeded model, over time.\n",
    "    inference_results = {}\n",
    "    kwargs = {}\n",
    "    with torch.no_grad():\n",
    "        out = ffn_model(coords, img=data)\n",
    "        pred = out['predicted']\n",
    "        intermediate_results = out[\"intermediate_results\"]\n",
    "        \n",
    "        if reshape:\n",
    "            # This reshapes the prediction into an image\n",
    "            pred = proc.process_outputs(\n",
    "                pred,input_img_shape=batch['data_shape'].squeeze().tolist(),\\\n",
    "                features_shape=features_shape,\\\n",
    "                patch_shape=cfg.data.patch_shape)\n",
    "\n",
    "    # Compute superpixels on the image\n",
    "    image_numpy = data[0].permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    rgb_cluster_map = compute_kmeans_clusters_in_rgb(image_numpy, num_rgb_clusters)\n",
    "\n",
    "    # Compute Gabor clusters map\n",
    "    image_pil_format = (data[0].clamp(0,1) * 255).permute(1,2,0).cpu().numpy().astype(np.uint8)\n",
    "    gabor_cluster_map = get_gabor_label_map(image_pil_format, num_gabor_clusters)\n",
    "\n",
    "    inference_results = {\n",
    "        \"data\": batch[\"data\"],\n",
    "        \"pred\": pred,\n",
    "        \"annotations\": annotations,\n",
    "        \"img_hw\": (H,W),\n",
    "        \"intermediate_results\": intermediate_results,\n",
    "        \"rgb_cluster_map\": rgb_cluster_map,\n",
    "        \"gabor_cluster_map\": gabor_cluster_map\n",
    "\n",
    "    }\n",
    "    \n",
    "    categories_in_frame = {}\n",
    "    for ann in annotations:\n",
    "        if ann[\"category_id\"] not in categories_in_frame:\n",
    "            categories_in_frame[ann[\"category_id\"]] = categories_dict[ann[\"category_id\"]]\n",
    "\n",
    "    categories_in_frame[-1] = categories_dict[-1]\n",
    "    object_categories = [v['name'] for k, v in categories_in_frame.items()]\n",
    "    categories_in_frame = [v for k, v in categories_in_frame.items()]\n",
    "    \n",
    "    return inference_results, categories_in_frame, object_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d4fdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_variables_for_image(inference_results, ffn_model, instance_to_ann_id_map, cell_stride_h, cell_stride_w, \n",
    "                                    instance_names):\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    (H,W) = inference_results[\"img_hw\"]\n",
    "    annotations = inference_results[\"annotations\"]\n",
    "\n",
    "    all_variables_for_image = {}\n",
    "    \n",
    "    num_regions = cell_stride_h * cell_stride_w\n",
    "    \n",
    "    # for img_idx, value in inference_results.items():\n",
    "    pred = inference_results[\"pred\"]\n",
    "    data = inference_results[\"data\"]\n",
    "    intermediate_results = inference_results[\"intermediate_results\"]\n",
    "    \n",
    "    rgb_cluster_map = inference_results[\"rgb_cluster_map\"]\n",
    "    gabor_cluster_map = inference_results[\"gabor_cluster_map\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeMLPContributions(\n",
    "        ffn_model, intermediate_results, (H,W)\n",
    "    )\n",
    "\n",
    "    layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, _, _, _ = compute_contrib_obj.compute_all_layer_mappings()\n",
    "\n",
    "    # Get instance contributions\n",
    "    layer_1_to_instance_contribs, layer_2_to_instance_contribs, layer_3_to_instance_contribs, \\\n",
    "        layer_1_instance_contrib_ratio_to_total, layer_2_instance_contrib_ratio_to_total, layer_3_instance_contrib_ratio_to_total, instance_areas \\\n",
    "            = get_instance_contribs(layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, annotations, instance_to_ann_id_map, instance_names)\n",
    "\n",
    "    # Get gridcell contributions\n",
    "    layer_1_to_gridcell_contribs, layer_2_to_gridcell_contribs, layer_3_to_gridcell_contribs, \\\n",
    "        layer_1_gridcell_contrib_ratio_to_total, layer_2_gridcell_contrib_ratio_to_total, layer_3_gridcell_contrib_ratio_to_total \\\n",
    "            = get_gridcell_contribs(layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, cell_stride_h, cell_stride_w)\n",
    "        \n",
    "    # Get RGB kmeans clustered contributions and normalize them. Also get variances within RGB clusters.\n",
    "    layer_1_to_rgb_cluster_contribs, layer_2_to_rgb_cluster_contribs, layer_3_to_rgb_cluster_contribs, \\\n",
    "        layer_1_rgb_cluster_contrib_ratio_to_total, layer_2_rgb_cluster_contrib_ratio_to_total, layer_3_rgb_cluster_contrib_ratio_to_total, rgb_cluster_areas \\\n",
    "            = get_rgb_cluster_contribs(layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, rgb_cluster_map)\n",
    "            \n",
    "    # Get RGB kmeans clustered contributions and normalize them. Also get variances within RGB clusters.\n",
    "    layer_1_to_gabor_cluster_contribs, layer_2_to_gabor_cluster_contribs, layer_3_to_gabor_cluster_contribs, \\\n",
    "        layer_1_gabor_cluster_contrib_ratio_to_total, layer_2_gabor_cluster_contrib_ratio_to_total, layer_3_gabor_cluster_contrib_ratio_to_total, gabor_cluster_areas \\\n",
    "            = get_gabor_cluster_contribs(layer_1_output_contrib, layer_2_output_contrib, layer_3_output_contrib, gabor_cluster_map)\n",
    "\n",
    "    # Beware, some of the neurons in MLP are dead (all 0 contribs). These are removed in normalization\n",
    "    all_variables_for_image = {\n",
    "        # \"instance_names\": instance_names,\n",
    "        \"layer_1_output_contrib\": torch.abs(layer_1_output_contrib),\n",
    "        \"layer_2_output_contrib\": torch.abs(layer_2_output_contrib),\n",
    "        \"layer_3_output_contrib\": torch.abs(layer_3_output_contrib),\n",
    "\n",
    "        # areas\n",
    "        \"instance_areas\": instance_areas,\n",
    "        \"rgb_cluster_areas\": rgb_cluster_areas,\n",
    "        \"gabor_cluster_areas\": gabor_cluster_areas,\n",
    "        \n",
    "        # per-patch contribution ratios\n",
    "        \"layer_3_instance_contrib_ratio_to_total\": layer_3_instance_contrib_ratio_to_total,\n",
    "        \"layer_2_instance_contrib_ratio_to_total\": layer_2_instance_contrib_ratio_to_total,\n",
    "        \"layer_1_instance_contrib_ratio_to_total\": layer_1_instance_contrib_ratio_to_total,\n",
    "        \n",
    "        \"layer_3_gridcell_contrib_ratio_to_total\": layer_3_gridcell_contrib_ratio_to_total,\n",
    "        \"layer_2_gridcell_contrib_ratio_to_total\": layer_2_gridcell_contrib_ratio_to_total,\n",
    "        \"layer_1_gridcell_contrib_ratio_to_total\": layer_1_gridcell_contrib_ratio_to_total,\n",
    "        \n",
    "        \"layer_3_rgb_cluster_contrib_ratio_to_total\": layer_3_rgb_cluster_contrib_ratio_to_total,\n",
    "        \"layer_2_rgb_cluster_contrib_ratio_to_total\": layer_2_rgb_cluster_contrib_ratio_to_total,\n",
    "        \"layer_1_rgb_cluster_contrib_ratio_to_total\": layer_1_rgb_cluster_contrib_ratio_to_total,\n",
    "\n",
    "        \"layer_3_gabor_cluster_contrib_ratio_to_total\": layer_3_gabor_cluster_contrib_ratio_to_total,\n",
    "        \"layer_2_gabor_cluster_contrib_ratio_to_total\": layer_2_gabor_cluster_contrib_ratio_to_total,\n",
    "        \"layer_1_gabor_cluster_contrib_ratio_to_total\": layer_1_gabor_cluster_contrib_ratio_to_total,\n",
    "        \n",
    "        \"num_instances_in_frame\": len(instance_areas),\n",
    "    }\n",
    "\n",
    "    return all_variables_for_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd8a82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_of_deltas(all_variables_for_image):\n",
    "\n",
    "    num_instances_in_frame = all_variables_for_image[\"num_instances_in_frame\"]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12,8), tight_layout=True)\n",
    "    \n",
    "    \n",
    "    layer_3_instance_variances = torch.var(all_variables_for_image[\"layer_3_instance_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_3_gridcell_variances = torch.var(all_variables_for_image[\"layer_3_gridcell_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_3_rgb_cluster_variances = torch.var(all_variables_for_image[\"layer_3_rgb_cluster_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_3_gabor_cluster_variances = torch.var(all_variables_for_image[\"layer_3_gabor_cluster_contrib_ratio_to_total\"], dim=-1)\n",
    "    \n",
    "    layer_2_instance_variances = torch.var(all_variables_for_image[\"layer_2_instance_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_2_gridcell_variances = torch.var(all_variables_for_image[\"layer_2_gridcell_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_2_rgb_cluster_variances = torch.var(all_variables_for_image[\"layer_2_rgb_cluster_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_2_gabor_cluster_variances = torch.var(all_variables_for_image[\"layer_2_gabor_cluster_contrib_ratio_to_total\"], dim=-1)\n",
    "    \n",
    "    layer_1_instance_variances = torch.var(all_variables_for_image[\"layer_1_instance_contrib_ratio_to_total\"], dim=-1)\n",
    "    layer_1_gridcell_variances = torch.var(all_variables_for_image[\"layer_1_gridcell_contrib_ratio_to_total\"], dim=-1) \n",
    "    layer_1_rgb_cluster_variances = torch.var(all_variables_for_image[\"layer_1_rgb_cluster_contrib_ratio_to_total\"], dim=-1) \n",
    "    layer_1_gabor_cluster_variances = torch.var(all_variables_for_image[\"layer_1_gabor_cluster_contrib_ratio_to_total\"], dim=-1)\n",
    "    \n",
    "    sorted_variance_layer_3_instance_contrib_ratio, layer_3_instance_sorted_indices = torch.sort(layer_3_instance_variances)\n",
    "    sorted_variance_layer_3_gridcell_contrib_ratio, layer_3_gridcell_sorted_indices = torch.sort(layer_3_gridcell_variances)\n",
    "    sorted_variance_layer_3_rgb_cluster_contrib_ratio, layer_3_rgb_cluster_sorted_indices = torch.sort(layer_3_rgb_cluster_variances)\n",
    "    sorted_variance_layer_3_gabor_cluster_contrib_ratio, layer_3_gabor_cluster_sorted_indices = torch.sort(layer_3_gabor_cluster_variances)\n",
    "    \n",
    "    sorted_variance_layer_2_instance_contrib_ratio, layer_2_instance_sorted_indices = torch.sort(layer_2_instance_variances)\n",
    "    sorted_variance_layer_2_gridcell_contrib_ratio, layer_2_gridcell_sorted_indices = torch.sort(layer_2_gridcell_variances)\n",
    "    sorted_variance_layer_2_rgb_cluster_contrib_ratio, layer_2_rgb_cluster_sorted_indices = torch.sort(layer_2_rgb_cluster_variances)\n",
    "    sorted_variance_layer_2_gabor_cluster_contrib_ratio, layer_2_gabor_cluster_sorted_indices = torch.sort(layer_2_gabor_cluster_variances)\n",
    "    \n",
    "    sorted_variance_layer_1_instance_contrib_ratio, layer_1_instance_sorted_indices = torch.sort(layer_1_instance_variances)\n",
    "    sorted_variance_layer_1_gridcell_contrib_ratio, layer_1_gridcell_sorted_indices = torch.sort(layer_1_gridcell_variances)\n",
    "    sorted_variance_layer_1_rgb_cluster_contrib_ratio, layer_1_rgb_cluster_sorted_indices = torch.sort(layer_1_rgb_cluster_variances)\n",
    "    sorted_variance_layer_1_gabor_cluster_contrib_ratio, layer_1_gabor_cluster_sorted_indices = torch.sort(layer_1_gabor_cluster_variances)\n",
    "    \n",
    "    labels = [\"Instances variance\", \"Grid cells variance\", \"RGB Cluster variance\", \"Gabor Cluster variance\"]\n",
    "    colors = ['r', 'g', 'b', 'm']\n",
    "    \n",
    "    # Plot layer 3\n",
    "    for idx, var in enumerate([sorted_variance_layer_3_instance_contrib_ratio, sorted_variance_layer_3_gridcell_contrib_ratio, sorted_variance_layer_3_rgb_cluster_contrib_ratio, sorted_variance_layer_3_gabor_cluster_contrib_ratio]):\n",
    "        axs[0].plot(var, label=labels[idx], c=colors[idx])        \n",
    "    axs[0].set_title(f\"Layer 3\")\n",
    "    \n",
    "    # Plot layer 2\n",
    "    for idx, var in enumerate([sorted_variance_layer_2_instance_contrib_ratio, sorted_variance_layer_2_gridcell_contrib_ratio, sorted_variance_layer_2_rgb_cluster_contrib_ratio, sorted_variance_layer_2_gabor_cluster_contrib_ratio]):\n",
    "        axs[1].plot(var, label=labels[idx], c=colors[idx])\n",
    "    axs[1].set_title(f\"Layer 2\")\n",
    "    \n",
    "    # Plot layer 1\n",
    "    for idx, var in enumerate([sorted_variance_layer_1_instance_contrib_ratio, sorted_variance_layer_1_gridcell_contrib_ratio, sorted_variance_layer_1_rgb_cluster_contrib_ratio, sorted_variance_layer_1_gabor_cluster_contrib_ratio]):\n",
    "        axs[2].plot(var, label=labels[idx], c=colors[idx])\n",
    "    axs[2].set_title(f\"Layer 1\")\n",
    "    \n",
    "    \n",
    "    fig.suptitle(f\"num_inst={num_instances_in_frame}, num_rgb_clust={num_rgb_clusters}, num_cells={cell_stride_h*cell_stride_w}\", fontweight=\"bold\")\n",
    "    \n",
    "    # Every subplot has the same legend, let us pick one \n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=6, bbox_to_anchor=(0.5, 1.05))\n",
    "    \n",
    "    sorted_deltas_dict = {\n",
    "            \"layer_1\": {\n",
    "                \"instances_deltas\": sorted_variance_layer_1_instance_contrib_ratio,\n",
    "                \"gridcells_deltas\": sorted_variance_layer_1_gridcell_contrib_ratio,\n",
    "                \"rgb_clusters_deltas\": sorted_variance_layer_1_rgb_cluster_contrib_ratio,\n",
    "                \"gabor_clusters_deltas\": sorted_variance_layer_1_gabor_cluster_contrib_ratio\n",
    "            }, \"layer_2\": {\n",
    "                \"instances_deltas\": sorted_variance_layer_2_instance_contrib_ratio,\n",
    "                \"gridcells_deltas\": sorted_variance_layer_2_gridcell_contrib_ratio,\n",
    "                \"rgb_clusters_deltas\": sorted_variance_layer_2_rgb_cluster_contrib_ratio,\n",
    "                \"gabor_clusters_deltas\": sorted_variance_layer_2_gabor_cluster_contrib_ratio\n",
    "            }, \"layer_3\": {\n",
    "                \"instances_deltas\": sorted_variance_layer_3_instance_contrib_ratio,\n",
    "                \"gridcells_deltas\": sorted_variance_layer_3_gridcell_contrib_ratio,\n",
    "                \"rgb_clusters_deltas\": sorted_variance_layer_3_rgb_cluster_contrib_ratio,\n",
    "                \"gabor_clusters_deltas\": sorted_variance_layer_3_gabor_cluster_contrib_ratio\n",
    "            }, \"sorted_indices\": {\n",
    "                \"layer_1\": {\n",
    "                    \"instances\": layer_1_instance_sorted_indices,\n",
    "                    \"gridcells\": layer_1_gridcell_sorted_indices,\n",
    "                    \"rgb_clusters\": layer_1_rgb_cluster_sorted_indices,\n",
    "                    \"gabor_clusters\": layer_1_gabor_cluster_sorted_indices\n",
    "                }, \"lsyer_2\": {\n",
    "                    \"instances\": layer_2_instance_sorted_indices,\n",
    "                    \"gridcells\": layer_2_gridcell_sorted_indices,\n",
    "                    \"rgb_clusters\": layer_2_rgb_cluster_sorted_indices,\n",
    "                    \"gabor_clusters\": layer_2_gabor_cluster_sorted_indices\n",
    "                }, \"layer_3\": {\n",
    "                    \"instances\": layer_3_instance_sorted_indices,\n",
    "                    \"gridcells\": layer_3_gridcell_sorted_indices,\n",
    "                    \"rgb_clusters\": layer_3_rgb_cluster_sorted_indices,\n",
    "                    \"gabor_clusters\": layer_3_gabor_cluster_sorted_indices\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return sorted_deltas_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc078181",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_patch_deltas_var_dict = {}\n",
    "\n",
    "# Cluster settings (for now set all equalish to num_instances)\n",
    "num_rgb_and_gabor_clusters_dict = {\n",
    "    \"0005\": 32,\n",
    "    \"26_cblDl5vCZnw\": 24\n",
    "}\n",
    "cell_stride_h_dict = {\n",
    "    \"0005\": 4,\n",
    "    \"26_cblDl5vCZnw\": 4\n",
    "}\n",
    "cell_stride_w_dict = {\n",
    "    \"0005\": 8, # 4*8 = 32\n",
    "    \"26_cblDl5vCZnw\": 6 # 4*6 = 24\n",
    "}\n",
    "\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        single_image_dataloader = dataloader_dict[dataset_name][vidname]\n",
    "        ffn_model = ffn_models_dict[dataset_name][vidname]\n",
    "        categories_dict = categories_dicts[dataset_name][vidname]\n",
    "        cfg = cfg_dict[dataset_name][vidname]\n",
    "\n",
    "        categories = list(categories_dict.values())\n",
    "        \n",
    "        num_rgb_clusters = num_rgb_and_gabor_clusters_dict[vidname]\n",
    "        num_gabor_clusters = num_rgb_and_gabor_clusters_dict[vidname]\n",
    "        cell_stride_h, cell_stride_w = cell_stride_h_dict[vidname], cell_stride_w_dict[vidname]\n",
    "        \n",
    "        inference_results, categories_in_frame, object_categories = compute_inference_results(\n",
    "            single_image_dataloader, ffn_model, cfg, categories_dict, num_rgb_clusters, num_gabor_clusters\n",
    "        )\n",
    "\n",
    "        inst_id_to_cat_and_inst_suffix, instance_to_ann_id_map, instance_names, object_to_instances_map, \\\n",
    "            obj_to_obj_name_idx, instance_names = get_instance_info(inference_results, object_categories, categories)\n",
    "\n",
    "        all_variables_for_image = compute_all_variables_for_image(\n",
    "            inference_results, ffn_model, instance_to_ann_id_map, cell_stride_h, cell_stride_w,\n",
    "            instance_names\n",
    "        )\n",
    "\n",
    "        sorted_deltas_dict = compute_variance_of_deltas(all_variables_for_image)\n",
    "\n",
    "        # For optical flow stuff, it might be easiest to \n",
    "        per_vid_patch_deltas_var_dict[vidname] = {\n",
    "            \"sorted_deltas_dict\" : sorted_deltas_dict,\n",
    "            \"cluster_info\": {\n",
    "                \"num_instances\": all_variables_for_image[\"num_instances_in_frame\"],\n",
    "                \"num_rgb_clusters\": num_rgb_clusters,\n",
    "                \"num_gabor_clusters\": num_gabor_clusters,\n",
    "                \"cell_stride_h\": cell_stride_h,\n",
    "                \"cell_stride_w\": cell_stride_w\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa4992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874d6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eed99b0-4275-41bc-a730-b8a53449219c",
   "metadata": {},
   "source": [
    "# Save dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = '../plotting_source_data/MLP/C-INRs_perhaps_care_about_objects'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(save_dir, f\"per_vid_patch_deltas_var_dict.pkl\"), 'wb') as f:\n",
    "    pickle.dump(per_vid_patch_deltas_var_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
