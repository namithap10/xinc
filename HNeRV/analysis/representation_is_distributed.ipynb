{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d071860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "from analysis_utils import *\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from get_mappings import ComputeContributions\n",
    "from model_all_analysis import HNeRV\n",
    "from vps_datasets import CityscapesVPSVideoDataSet, VIPSegVideoDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad07404",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"cityscapes\", \"vipseg\"]\n",
    "vidnames = {\n",
    "    \"cityscapes\": [\"0005\", \"0175\"],\n",
    "    \"vipseg\": [\"12_n-ytHkMceew\", \"26_cblDl5vCZnw\"],\n",
    "}\n",
    "\n",
    "args_dict = {}\n",
    "dataloader_dict = {}\n",
    "weights_dict = {}\n",
    "models_dict = {}\n",
    "categories_dicts = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    weights_dict[dataset_name] = {}\n",
    "    args_dict[dataset_name] = {}\n",
    "    dataloader_dict[dataset_name] = {}\n",
    "    models_dict[dataset_name] = {}\n",
    "    categories_dicts[dataset_name] = {}\n",
    "\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        weights_dict[dataset_name][vidname] = \"path/to/checkpoint/\"\n",
    "\n",
    "        args = load_model_args()\n",
    "\n",
    "        args.weight = os.path.join(\n",
    "            weights_dict[dataset_name][vidname], f\"model_best.pth\"\n",
    "        )\n",
    "        args.crop_list = \"-1\" if dataset_name == \"cityscapes\" else \"640_1280\"\n",
    "\n",
    "        model = HNeRV(args)\n",
    "        model = load_model_checkpoint(model, args)\n",
    "        models_dict[dataset_name][vidname] = model\n",
    "\n",
    "        args, categories_dicts[dataset_name][vidname] = load_dataset_specific_args(\n",
    "            args, dataset_name, vidname\n",
    "        )\n",
    "\n",
    "        args_dict[dataset_name][vidname] = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed44ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "\n",
    "        args = args_dict[dataset_name][vidname]\n",
    "\n",
    "        if dataset_name == \"cityscapes\":\n",
    "            full_dataset = CityscapesVPSVideoDataSet(args)\n",
    "        else:\n",
    "            full_dataset = VIPSegVideoDataSet(args)\n",
    "\n",
    "        sampler = (\n",
    "            torch.utils.data.distributed.DistributedSampler(full_dataset)\n",
    "            if args.distributed\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        args.final_size = full_dataset.final_size\n",
    "        args.full_data_length = len(full_dataset)\n",
    "        split_num_list = [int(x) for x in args.data_split.split(\"_\")]\n",
    "        train_ind_list, args.val_ind_list = data_split(\n",
    "            list(range(args.full_data_length)), split_num_list, args.shuffle_data, 0\n",
    "        )\n",
    "\n",
    "        train_dataset = Subset(full_dataset, train_ind_list)\n",
    "        train_sampler = (\n",
    "            torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "            if args.distributed\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batchSize,\n",
    "            shuffle=False,\n",
    "            num_workers=args.workers,\n",
    "            pin_memory=True,\n",
    "            sampler=train_sampler,\n",
    "            drop_last=True,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "        )\n",
    "\n",
    "        dataloader_dict[dataset_name][vidname] = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4c9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inference_results(\n",
    "    dataset_name, train_dataloader, model, args, first_frame_only=True\n",
    "):\n",
    "\n",
    "    if dataset_name == \"vipseg\":\n",
    "        # Sample few frames\n",
    "        num_indices = len(train_dataloader) * args.b\n",
    "        num_samples = 6\n",
    "        sampled_img_indices = [\n",
    "            i * (num_indices - 1) // (num_samples - 1) for i in range(num_samples)\n",
    "        ]\n",
    "\n",
    "    inference_results = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in train_dataloader:\n",
    "            img_data, norm_idx, img_idx = (\n",
    "                batch[\"img\"].to(\"cuda\"),\n",
    "                batch[\"norm_idx\"].to(\"cuda\"),\n",
    "                batch[\"idx\"].to(\"cuda\"),\n",
    "            )\n",
    "\n",
    "            if dataset_name == \"vipseg\" and (img_idx not in sampled_img_indices):\n",
    "                continue\n",
    "\n",
    "            images = batch[\"img\"].cuda()\n",
    "            _, _, _, decoder_results, img_out = model(norm_idx)\n",
    "\n",
    "            # Save all input-output information related to annotated images\n",
    "            inference_results[img_idx.item()] = {\n",
    "                \"decoder_results\": decoder_results,\n",
    "                \"img_out\": img_out,\n",
    "                \"img_gt\": images[0],\n",
    "            }\n",
    "\n",
    "            if first_frame_only:\n",
    "                break\n",
    "    return inference_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdde7d",
   "metadata": {},
   "source": [
    "# Pixels Per Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a26fcc2-72dd-470b-8015-edc10414d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixels_per_neuron(inference_results, model, args):\n",
    "    img_idx = 0\n",
    "    img_out = inference_results[img_idx][\"img_out\"]\n",
    "    decoder_results = inference_results[img_idx][\"decoder_results\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeContributions(\n",
    "        model, args, decoder_results, img_out.detach().clone()[0]\n",
    "    )\n",
    "\n",
    "    head_layer_output_contrib = compute_contrib_obj.compute_head_mappings()\n",
    "    nerv_blk_3_output_contrib, _ = (\n",
    "        compute_contrib_obj.compute_last_nerv_block_mappings()\n",
    "    )\n",
    "    # Flatten by kernels by pixels\n",
    "    head_layer_output_contrib_abs = (\n",
    "        torch.abs(head_layer_output_contrib).flatten(0, 1).flatten(1, 2)\n",
    "    )\n",
    "    nerv_blk_3_output_contrib_abs = (\n",
    "        torch.abs(nerv_blk_3_output_contrib).flatten(0, 1).flatten(1, 2)\n",
    "    )\n",
    "    # Sum contribs across kernels for each pixel\n",
    "    head_contribs_fraction = (\n",
    "        head_layer_output_contrib_abs / head_layer_output_contrib_abs.sum(dim=0)\n",
    "    )\n",
    "    blk_3_contribs_fraction = (\n",
    "        nerv_blk_3_output_contrib_abs / nerv_blk_3_output_contrib_abs.sum(dim=0)\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "    num_pixels_with_meaningful_contrib_dict = {}\n",
    "\n",
    "    head_num_pixels_with_meaningful_contrib = (\n",
    "        head_contribs_fraction > (1 / head_layer_output_contrib_abs.size(0))\n",
    "    ).sum(dim=1)\n",
    "    blk_3_num_pixels_with_meaningful_contrib = (\n",
    "        blk_3_contribs_fraction > (1 / nerv_blk_3_output_contrib_abs.size(0))\n",
    "    ).sum(dim=1)\n",
    "    head_num_pixels_with_meaningful_contrib, _ = torch.sort(\n",
    "        head_num_pixels_with_meaningful_contrib\n",
    "    )\n",
    "    blk_3_num_pixels_with_meaningful_contrib, _ = torch.sort(\n",
    "        blk_3_num_pixels_with_meaningful_contrib\n",
    "    )\n",
    "\n",
    "    axs.plot(\n",
    "        head_num_pixels_with_meaningful_contrib,\n",
    "        color=\"b\",\n",
    "        label=f\"Threshold - 1/num_kernels\",\n",
    "    )\n",
    "    axs.plot(\n",
    "        blk_3_num_pixels_with_meaningful_contrib,\n",
    "        color=\"g\",\n",
    "        label=f\"Threshold - 1/num_kernels\",\n",
    "    )\n",
    "    axs.set_xlabel(\"Neuron index\")\n",
    "    axs.set_ylabel(\"Number of pixels\")\n",
    "    axs.set_title(f\"Head layer\")\n",
    "\n",
    "    num_pixels_with_meaningful_contrib_dict = {\n",
    "        \"head\": head_num_pixels_with_meaningful_contrib,\n",
    "        \"blk_3\": blk_3_num_pixels_with_meaningful_contrib,\n",
    "    }\n",
    "\n",
    "    legend, handles = axs.get_legend_handles_labels()\n",
    "    fig.legend(legend, handles)\n",
    "    fig.suptitle(\n",
    "        f\"Frame {img_idx} - Number of pixels contributed to by neuron (above Threshold)\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    return num_pixels_with_meaningful_contrib_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc7983",
   "metadata": {},
   "source": [
    "# Neurons Per Pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e3fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrib_thresh_using_auc(abs_contrib_map, target_area=0.05):\n",
    "    total_sum = abs_contrib_map.sum()\n",
    "    cutoff_contrib_sum = total_sum * (1 - target_area)\n",
    "\n",
    "    sorted_contributions = abs_contrib_map.flatten()\n",
    "    sorted_indices = torch.argsort(sorted_contributions, descending=True)\n",
    "    cum_sum = torch.cumsum(sorted_contributions[sorted_indices], dim=0)\n",
    "\n",
    "    idx = torch.nonzero(cum_sum >= cutoff_contrib_sum, as_tuple=False)[0, 0].item()\n",
    "    chosen_thresh = sorted_contributions[sorted_indices][idx]\n",
    "\n",
    "    return chosen_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbcded1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neurons_per_pixel_heatmap(vidname, inference_results, model, args):\n",
    "    img_idx = 0\n",
    "    gt_img = inference_results[img_idx][\"img_gt\"]\n",
    "    img_out = inference_results[img_idx][\"img_out\"]\n",
    "    decoder_results = inference_results[img_idx][\"decoder_results\"]\n",
    "\n",
    "    # Get model contributions\n",
    "    compute_contrib_obj = ComputeContributions(\n",
    "        model, args, decoder_results, img_out.detach().clone()[0]\n",
    "    )\n",
    "\n",
    "    # The following can be extended to inner blocks of NeRV as well\n",
    "    head_layer_output_contrib = compute_contrib_obj.compute_head_mappings()\n",
    "    nerv_blk_3_output_contrib, _ = (\n",
    "        compute_contrib_obj.compute_last_nerv_block_mappings()\n",
    "    )\n",
    "    head_layer_output_contrib_abs = torch.abs(head_layer_output_contrib).flatten(0, 1)\n",
    "    nerv_blk_3_output_contrib_abs = torch.abs(nerv_blk_3_output_contrib).flatten(0, 1)\n",
    "\n",
    "    target_areas = [0.1, 0.5]\n",
    "\n",
    "    num_kernels_with_meaningful_contrib = {}\n",
    "\n",
    "    for target_area in target_areas:\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "        # Compute head and block 3 kernel threshold contributions\n",
    "        head_thresh = compute_contrib_thresh_using_auc(\n",
    "            abs_contrib_map=head_layer_output_contrib_abs, target_area=target_area\n",
    "        )\n",
    "        blk_3_thresh = compute_contrib_thresh_using_auc(\n",
    "            abs_contrib_map=nerv_blk_3_output_contrib_abs, target_area=target_area\n",
    "        )\n",
    "\n",
    "        # Find total number of neurons over threshold and divide by total number\n",
    "        head_num_kernels_with_meaningful_contrib = (\n",
    "            head_layer_output_contrib_abs > head_thresh\n",
    "        ).sum(dim=0) / head_layer_output_contrib_abs.size(0)\n",
    "        blk_3_num_kernels_with_meaningful_contrib = (\n",
    "            nerv_blk_3_output_contrib_abs > blk_3_thresh\n",
    "        ).sum(dim=0) / nerv_blk_3_output_contrib_abs.size(0)\n",
    "\n",
    "        axs[0].imshow(torch.clamp(gt_img, 0, 1).permute(1, 2, 0).cpu().numpy())\n",
    "        axs[0].set_title(\"Ground Truth Image\")\n",
    "        axs[1].set_title(f\"Head Layer Heatmap - thresh={head_thresh:.4f}\")\n",
    "        axs[2].set_title(f\"Block 3 Heatmap - thresh={blk_3_thresh:.4f}\")\n",
    "        for ax in axs:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        num_kernels_with_meaningful_contrib[target_area] = {\n",
    "            \"head\": head_num_kernels_with_meaningful_contrib,\n",
    "            \"blk_3\": blk_3_num_kernels_with_meaningful_contrib,\n",
    "        }\n",
    "\n",
    "        fig.suptitle(\"Neurons per Pixel Heatmap\", y=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    return num_kernels_with_meaningful_contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4196",
   "metadata": {},
   "source": [
    "Analyze each video and save the raw values for downstream visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f537d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_vid_num_pixels_with_meaningful_contrib = {}\n",
    "per_vid_num_kernels_with_meaningful_contrib = {}\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for vidname in vidnames[dataset_name]:\n",
    "        train_dataloader = dataloader_dict[dataset_name][vidname]\n",
    "        model = models_dict[dataset_name][vidname]\n",
    "        categories_dict = categories_dicts[dataset_name][vidname]\n",
    "        args = args_dict[dataset_name][vidname]\n",
    "\n",
    "        inference_results = compute_inference_results(\n",
    "            dataset_name, vidname, train_dataloader, model, args\n",
    "        )\n",
    "        per_vid_num_pixels_with_meaningful_contrib[vidname] = plot_pixels_per_neuron(\n",
    "            vidname, inference_results, model, args\n",
    "        )\n",
    "        per_vid_num_kernels_with_meaningful_contrib[vidname] = (\n",
    "            plot_neurons_per_pixel_heatmap(vidname, inference_results, model, args)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1970799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dir = \"../analysis_data/NeRV/representation_is_distributed\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(\n",
    "    os.path.join(save_dir, f\"per_vid_num_pixels_with_meaningful_contrib.pkl\"), \"wb\"\n",
    ") as f:\n",
    "    pickle.dump(per_vid_num_pixels_with_meaningful_contrib, f)\n",
    "with open(\n",
    "    os.path.join(save_dir, f\"per_vid_num_kernels_with_meaningful_contrib.pkl\"), \"wb\"\n",
    ") as f:\n",
    "    pickle.dump(per_vid_num_kernels_with_meaningful_contrib, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
